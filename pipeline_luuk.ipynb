{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f97a5615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "VERBOSE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a03eb34",
   "metadata": {},
   "source": [
    "## Lisanne methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a89b0286",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCHES_DIR   = Path(r\"C:\\Users\\luukn\\AIMI_MONKEY2\\monkey-training\\patches_newest\\pas-cpg\")\n",
    "JSON_DIR      = Path(r\"C:\\Users\\luukn\\AIMI_MONKEY2\\monkey-training\\annotations\\json_mm\")\n",
    "OUTPUT_JSON   = Path(r\"C:\\Users\\luukn\\AIMI_MONKEY2\\monkey-training\\training_output\\submission.json\")\n",
    "\n",
    "PATCH_SIZE    = 256        # as in extract_patch call\n",
    "BBOX_SIZE     = 32         # size of the box we teach the model to find\n",
    "DEVICE        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES   = 2          # background + inflammatory\n",
    "LR            = 1e-4\n",
    "EPOCHS        = 12\n",
    "BATCH_SIZE    = 4\n",
    "PATIENCE       = 3       # early stop if no val recall improvement\n",
    "\n",
    "# ── TRANSFORMS ────────────────────────────────────────────────────────────────\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "# ─── HELPERS ───────────────────────────────────────────────────────────────────\n",
    "def parse_slide_jsons(json_dir):\n",
    "    \"\"\"\n",
    "    Return dict mapping (slide_id, class_name) -> list of (x,y) ROI coords (pixels).\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for jf in json_dir.glob(\"A_*_*.json\"):\n",
    "        parts = jf.stem.split(\"_\")        # [\"A\",\"P000001\",\"inflammatory-cells\"]\n",
    "        slide = parts[1]\n",
    "        cls   = \"_\".join(parts[2:])\n",
    "        data  = json.loads(jf.read_text())\n",
    "        pts   = [pt[\"point\"] for pt in data[\"points\"]]\n",
    "        d[(slide, cls)] = pts\n",
    "    return d\n",
    "\n",
    "def build_patch_annotations(patches_dir: Path, coords_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    For each patch image under patches_dir/<class>/*.png:\n",
    "      - We ignore the JSON indexing entirely\n",
    "      - We place exactly one BBOX_SIZE×BBOX_SIZE box centered in the PATCH_SIZE×PATCH_SIZE patch\n",
    "    \"\"\"\n",
    "    annots = {}\n",
    "    half = PATCH_SIZE // 2\n",
    "    off  = half - BBOX_SIZE // 2\n",
    "\n",
    "    for img_path in patches_dir.glob(\"*/*.png\"):\n",
    "        name = img_path.name  # e.g. \"P000009_inflammatory-cells_patch0.png\"\n",
    "\n",
    "        # simply assign a centred box for every patch\n",
    "        x1 = off\n",
    "        y1 = off\n",
    "        x2 = off + BBOX_SIZE\n",
    "        y2 = off + BBOX_SIZE\n",
    "        annots[name] = [[x1, y1, x2, y2]]\n",
    "\n",
    "    return annots\n",
    "\n",
    "# ─── DATASET ────────────────────────────────────────────────────────────────────\n",
    "class PatchDetectDataset(Dataset):\n",
    "    def __init__(self, patches_dir, annots, transform=None):\n",
    "        self.annots  = annots\n",
    "        self.transform = transform\n",
    "        self.patches_dir = Path(patches_dir)\n",
    "        self.names = list(annots.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.names[idx]\n",
    "        img_paths = list(self.patches_dir.rglob(name))  # recursively find patch by exact filename\n",
    "        if not img_paths:\n",
    "            raise FileNotFoundError(f\"Could not find patch {name} in {self.patches_dir}\")\n",
    "        img = Image.open(img_paths[0]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        boxes  = torch.tensor(self.annots[name], dtype=torch.float32)\n",
    "        labels = torch.ones((boxes.size(0),), dtype=torch.int64)  # all class=1\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "        return img, target\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abe23256",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "build_patch_annotations() got an unexpected keyword argument 'box_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Prepare annots for train splits\u001b[39;00m\n\u001b[0;32m      5\u001b[0m train_splits \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpas-original\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpas-diagnostic\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m train_annots \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_patch_annotations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotations_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m PatchDetectDataset(root_dir, train_annots, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Prepare annots for test split\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: build_patch_annotations() got an unexpected keyword argument 'box_size'"
     ]
    }
   ],
   "source": [
    "root_dir = Path(r\"C:\\Users\\luukn\\AIMI_MONKEY2\\monkey-training\\patches_newest\")\n",
    "annotations_dir = r\"C:\\Users\\luukn\\AIMI_MONKEY2\\monkey-training\\annotations\\json_mm\"\n",
    "\n",
    "# Prepare annots for train splits\n",
    "train_splits = ['pas-original', 'pas-diagnostic']\n",
    "train_annots = build_patch_annotations(root_dir, annotations_dir, train_splits, box_size=32)\n",
    "train_ds = PatchDetectDataset(root_dir, train_annots, transform=transform)\n",
    "\n",
    "# Prepare annots for test split\n",
    "test_splits = ['cpg_test']\n",
    "test_annots = build_patch_annotations(root_dir, annotations_dir, test_splits, box_size=32)\n",
    "test_ds = PatchDetectDataset(root_dir, test_annots, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4f2e009",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m ds \u001b[38;5;241m=\u001b[39m PatchDetectDataset(PATCHES_DIR, patch_annots, transform\u001b[38;5;241m=\u001b[39mtf)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True,\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#                    collate_fn=collate_fn, num_workers=2)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m dl \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ← no subprocesses\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# 4) Set up Faster R‑CNN\u001b[39;00m\n\u001b[0;32m     24\u001b[0m sizes \u001b[38;5;241m=\u001b[39m ((\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m64\u001b[39m),)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m5\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:376\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 376\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:164\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    166\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "coords_dict = parse_slide_jsons(JSON_DIR)\n",
    "\n",
    "# 2) Build per-patch annotation dict\n",
    "patch_annots = build_patch_annotations(PATCHES_DIR, coords_dict)\n",
    "\n",
    "# 3) Create Dataset & DataLoader\n",
    "tf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "ds = PatchDetectDataset(PATCHES_DIR, patch_annots, transform=tf)\n",
    "#dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "#                    collate_fn=collate_fn, num_workers=2)\n",
    "\n",
    "dl = DataLoader(\n",
    "    ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,    # ← no subprocesses\n",
    ")\n",
    "\n",
    "# 4) Set up Faster R‑CNN\n",
    "sizes = ((16,32,64),)*5\n",
    "ratios= ((1.0,),)*5\n",
    "anchor_gen = AnchorGenerator(sizes=sizes, aspect_ratios=ratios)\n",
    "\n",
    "# 2) build model\n",
    "weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn(weights=weights,\n",
    "                                    rpn_anchor_generator=anchor_gen)\n",
    "#swap heads\n",
    "in_feats = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_feats, NUM_CLASSES)\n",
    "model.to(DEVICE)\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606df550",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbf7b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InflammatoryCellsDataset(Dataset):\n",
    "    def __init__(self, root_dir, annotations_dir, splits, transforms=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.annotations_dir = Path(annotations_dir)\n",
    "        self.splits = splits\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.image_paths = []\n",
    "        self.annotation_paths = []\n",
    "\n",
    "        for split in splits:\n",
    "            image_dir = self.root_dir / split / \"images\"\n",
    "            ann_dir = self.annotations_dir / split / \"annotations\"\n",
    "\n",
    "            for image_path in image_dir.glob(\"*.png\"):\n",
    "                ann_path = ann_dir / image_path.name.replace(\".png\", \".json\")\n",
    "                if ann_path.exists():\n",
    "                    self.image_paths.append(image_path)\n",
    "                    self.annotation_paths.append(ann_path)\n",
    "\n",
    "        assert len(self.image_paths) == len(self.annotation_paths), \"Mismatch in images and annotations\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.image_paths[idx]\n",
    "        ann_path = self.annotation_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Load annotations\n",
    "        with open(ann_path) as f:\n",
    "            ann_data = json.load(f)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for item in ann_data:\n",
    "            boxes.append(item[\"bbox\"])\n",
    "            labels.append(1)  # Binary label: 1 = inflammatory cell\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([idx])\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float32))\n",
    "    if train:\n",
    "        # add data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "\n",
    "def get_model(num_classes=2):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device, score_thresh=0.5):\n",
    "    model.eval()\n",
    "    total = detected = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "            for i, out in enumerate(outputs):\n",
    "                total += len(targets[i]['boxes'])\n",
    "                detected += (out['scores'] > score_thresh).sum().item()\n",
    "    return detected, total\n",
    "\n",
    "# subsample train, val, and test datasets for given fraction\n",
    "def subsample_dataset(dataset, fraction):\n",
    "    indices = torch.randperm(len(dataset))[:int(len(dataset) * fraction)]\n",
    "    return torch.utils.data.Subset(dataset, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a189d5e0",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42567275",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r\"C:\\Users\\luukn\\AIMI_MONKEY2\\monkey-training\\patches_newest\"\n",
    "annotations_dir = r\"C:\\Users\\luukn\\AIMI_MONKEY2\\monkey-training\\annotations\\json_mm\"\n",
    "\n",
    "# Prepare combined train splits\n",
    "train_splits = ['pas-original', 'pas-diagnostic']\n",
    "full_train = InflammatoryCellsDataset(root_dir, annotations_dir, splits=train_splits, transforms=get_transform(True))\n",
    "# 80/20 train/val split\n",
    "train_size = int(0.8 * len(full_train))\n",
    "val_size = len(full_train) - train_size\n",
    "train_ds, val_ds = random_split(full_train, [train_size, val_size])\n",
    "\n",
    "# PAS-CPG test set\n",
    "test_ds = InflammatoryCellsDataset(root_dir, annotations_dir, splits=['cpg_test'], transforms=get_transform(False))\n",
    "\n",
    "fraction = 0.1  # 1% of the dataset\n",
    "train_ds = subsample_dataset(train_ds, fraction)  # 10% of training data\n",
    "val_ds = subsample_dataset(val_ds, fraction)    # 10% of validation data\n",
    "test_ds = subsample_dataset(test_ds, fraction)  # 10% of test data\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader   = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader  = DataLoader(test_ds, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = get_model(num_classes=2)\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71d176f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 316 samples (80% split from: ['pas-original', 'pas-diagnostic']\n",
      "Validating on 79 samples (20% split from: ['pas-original', 'pas-diagnostic'])\n",
      "Evaluating on 102 samples from split: pas-cpg\n"
     ]
    }
   ],
   "source": [
    "if VERBOSE:\n",
    "    print(f\"Training on {len(train_loader)} samples (80% split from: {train_splits}\")\n",
    "    print(f\"Validating on {len(val_loader)} samples (20% split from: {train_splits})\")\n",
    "    print(f\"Evaluating on {len(test_loader)} samples from split: pas-cpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e663ae74",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f2e164f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 316/316 [1:08:49<00:00, 13.07s/it]\n",
      "Epoch 2/10:  72%|███████▏  | 228/316 [59:00<22:46, 15.53s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m images \u001b[38;5;241m=\u001b[39m [img\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[0;32m      8\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m----> 9\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n\u001b[1;32m--> 104\u001b[0m proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroi_heads(features, proposals, images\u001b[38;5;241m.\u001b[39mimage_sizes, targets)\n\u001b[0;32m    106\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\detection\\rpn.py:361\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[1;34m(self, images, features, targets)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;66;03m# RPN uses all feature maps that are available\u001b[39;00m\n\u001b[0;32m    360\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(features\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m--> 361\u001b[0m objectness, pred_bbox_deltas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    362\u001b[0m anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchor_generator(images, features)\n\u001b[0;32m    364\u001b[0m num_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(anchors)\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\detection\\rpn.py:75\u001b[0m, in \u001b[0;36mRPNHead.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     73\u001b[0m bbox_reg \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[1;32m---> 75\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     logits\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_logits(t))\n\u001b[0;32m     77\u001b[0m     bbox_reg\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbbox_pred(t))\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history = {'train_loss': [], 'val_detected_rate': []}\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += losses.item()\n",
    "        \n",
    "    # Validation detection rate\n",
    "    # detected, total = evaluate(model, val_loader, device)\n",
    "    # detect_rate = detected / total if total > 0 else 0\n",
    "    # history['train_loss'].append(epoch_loss)\n",
    "    # history['val_detected_rate'].append(detect_rate)\n",
    "    # print(f\"Epoch {epoch+1}: Loss={epoch_loss:.2f}, Val detect rate={detect_rate:.3f}\")\n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), \"FasterRCNN1.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c18dc",
   "metadata": {},
   "source": [
    "## Model Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840989b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on 4 patches from: C:\\Users\\luukn\\AIMI_MONKEY2\\monkey-training\\patches_newest\\cpg_test\n",
      "['C:\\\\Users\\\\luukn\\\\AIMI_MONKEY2\\\\monkey-training\\\\patches_newest\\\\cpg_test\\\\C_P000038_PAS_CPG_x22528_y134912.png', 'C:\\\\Users\\\\luukn\\\\AIMI_MONKEY2\\\\monkey-training\\\\patches_newest\\\\cpg_test\\\\D_P000001_PAS_CPG_x12032_y56064.png', 'C:\\\\Users\\\\luukn\\\\AIMI_MONKEY2\\\\monkey-training\\\\patches_newest\\\\cpg_test\\\\B_P000014_PAS_CPG_x36608_y71424.png', 'C:\\\\Users\\\\luukn\\\\AIMI_MONKEY2\\\\monkey-training\\\\patches_newest\\\\cpg_test\\\\D_P000001_PAS_CPG_x20736_y56832.png']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing patches:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing patches:  25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {'boxes': tensor([[255.9685, 146.1843, 255.9998, 150.7722],\n",
      "        [255.9553, 147.3349, 255.9998, 151.9171],\n",
      "        [255.9650, 176.8546, 255.9998, 181.4720],\n",
      "        [255.9965, 154.5183, 256.0000, 159.5017],\n",
      "        [255.9832,  51.3942, 255.9999,  56.8830]]), 'labels': tensor([1, 1, 1, 1, 1]), 'scores': tensor([0.2273, 0.2273, 0.2273, 0.2273, 0.2273])}\n",
      "Found 5 boxes with scores: [0.2273423820734024, 0.2273423820734024, 0.2273423820734024, 0.2273423820734024, 0.2273423820734024]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing patches:  50%|█████     | 2/4 [00:02<00:02,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {'boxes': tensor([[255.9730,  38.5137, 255.9999,  43.7299],\n",
      "        [255.9590,  37.2358, 255.9998,  42.5663]]), 'labels': tensor([1, 1]), 'scores': tensor([0.2273, 0.2273])}\n",
      "Found 2 boxes with scores: [0.2273423820734024, 0.2273423820734024]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing patches:  75%|███████▌  | 3/4 [00:03<00:01,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {'boxes': tensor([[255.9903, 197.4065, 256.0000, 204.6797],\n",
      "        [255.9637, 156.2251, 255.9998, 163.9164],\n",
      "        [255.9838, 154.8836, 255.9999, 162.5872],\n",
      "        [255.9843, 150.1323, 255.9999, 157.5981],\n",
      "        [255.9762, 189.7423, 255.9999, 197.2328],\n",
      "        [255.9896, 143.6749, 256.0000, 151.0083],\n",
      "        [255.9929, 142.3131, 256.0000, 149.7149],\n",
      "        [255.9814, 183.3926, 255.9999, 190.8801],\n",
      "        [255.9900, 188.3891, 256.0000, 195.9746],\n",
      "        [255.9934, 184.5830, 256.0000, 192.1342],\n",
      "        [255.9612, 129.9566, 255.9998, 137.9367],\n",
      "        [255.9640, 138.3562, 255.9998, 146.0214],\n",
      "        [255.9720, 206.0090, 255.9999, 214.3660],\n",
      "        [255.9852, 204.6808, 255.9999, 213.0072],\n",
      "        [255.9680, 135.3984, 255.9998, 143.2702],\n",
      "        [255.9816, 211.3675, 255.9999, 219.6600],\n",
      "        [255.9690,  42.8121, 255.9998,  50.4988],\n",
      "        [255.9630, 214.3173, 255.9998, 222.5139],\n",
      "        [255.9884, 218.1399, 255.9999, 226.3166],\n",
      "        [255.9773, 216.9089, 255.9999, 225.0610],\n",
      "        [255.9911, 105.1465, 256.0000, 109.8298],\n",
      "        [255.9735,  97.7821, 255.9999, 102.1478],\n",
      "        [255.9804, 228.6893, 255.9999, 237.3031]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'scores': tensor([0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,\n",
      "        0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273, 0.2273,\n",
      "        0.2273, 0.2273, 0.2273, 0.2273, 0.2273])}\n",
      "Found 23 boxes with scores: [0.2273423820734024, 0.2273423820734024, 0.2273423820734024, 0.2273423820734024, 0.2273423820734024, 0.2273423820734024, 0.2273423820734024, 0.2273423820734024, 0.2273423820734024, 0.2273423820734024, 0.2273423820734024, 0.2273423820734024, 0.2273423820734024, 0.2273423820734024, 0.2273423820734024, 0.2273423820734024, 0.2273423820734024, 0.2273423820734024, 0.2273423820734024, 0.2273423820734024, 0.2273423820734024, 0.2273423820734024, 0.2273423820734024]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing patches: 100%|██████████| 4/4 [00:04<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: {'boxes': tensor([[255.9676,   3.5159, 255.9998,   7.6344],\n",
      "        [255.9813,  87.8449, 255.9999,  95.3485],\n",
      "        [255.9913, 131.2984, 256.0000, 136.6867]]), 'labels': tensor([1, 1, 1]), 'scores': tensor([0.2273, 0.2273, 0.2273])}\n",
      "Found 3 boxes with scores: [0.2273423820734024, 0.2273423820734024, 0.2273423820734024]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_directory = r\"C:\\Users\\luukn\\AIMI_MONKEY2\\monkey-training\\patches_newest\\cpg_test\"\n",
    "test_patches = glob.glob(os.path.join(test_directory, \"*.png\"))\n",
    "\n",
    "# obtain random fraction of test patches\n",
    "random.shuffle(test_patches)\n",
    "test_patches = test_patches[:int(len(test_patches) * 0.1)]  # 10% of test patches\n",
    "\n",
    "if VERBOSE:\n",
    "    print(f\"Testing on {len(test_patches)} patches from: {test_directory}\")\n",
    "\n",
    "model.eval()\n",
    "SCORE_THRESH = 0.5\n",
    "\n",
    "print(test_patches)\n",
    "\n",
    "results = {}  # will map: filename → list of (x1,y1,x2,y2,score)\n",
    "with torch.no_grad():\n",
    "    for img_path in tqdm(test_patches, desc=\"Testing patches\"):\n",
    "        # load & preprocess\n",
    "        img_orig = Image.open(img_path).convert(\"RGB\")\n",
    "        transform = FasterRCNN_ResNet50_FPN_Weights.DEFAULT.transforms()\n",
    "        inp = transform(img_orig).to(device)\n",
    "\n",
    "        # images = [img.to(device) for img in images]\n",
    "        # outputs = model(images)\n",
    "        # print(outputs)\n",
    "\n",
    "        # forward\n",
    "        out = model([inp])\n",
    "        print(f\"Output: {out}\")\n",
    "        boxes  = out[\"boxes\"].cpu().tolist()\n",
    "        scores = out[\"scores\"].cpu().tolist()\n",
    "        print(f\"Found {len(boxes)} boxes with scores: {scores}\")\n",
    "\n",
    "        # filter low‐confidence\n",
    "        keep = [i for i,s in enumerate(scores) if s>=SCORE_THRESH]\n",
    "        detections = []\n",
    "        for i in keep:\n",
    "            x1,y1,x2,y2 = boxes[i]\n",
    "            sc = scores[i]\n",
    "            detections.append((x1,y1,x2,y2,sc))\n",
    "\n",
    "        # store\n",
    "        img_name = os.path.splitext(Path(img_path).name)[0]\n",
    "        results[img_name] = detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea0c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_JSON = Path(\"detection_results_12.json\")\n",
    "\n",
    "# 1) Write to disk\n",
    "with OUT_JSON.open(\"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Wrote {len(results)} entries to {OUT_JSON!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac9397df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C_P000038_PAS_CPG_x22528_y134912': [],\n",
       " 'D_P000001_PAS_CPG_x12032_y56064': [],\n",
       " 'B_P000014_PAS_CPG_x36608_y71424': [],\n",
       " 'D_P000001_PAS_CPG_x20736_y56832': []}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876eb442",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21b95e6",
   "metadata": {},
   "source": [
    "### Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e7ae77d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAHqCAYAAADVi/1VAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB0XElEQVR4nO3deVxU1f/H8feAsiq4Amq4pH3dl0JBzFz5hksaqeVWILmUaVZY38Rc0r5Gq1G5tbhkZSJmWlluuKVRKuaWS1ouuQAuCYkKCvf3hz/m28SgaHAH5fV8PO6j5txz73zOmdF7/My551oMwzAEAAAAAAAAmMjJ0QEAAAAAAACg5CEpBQAAAAAAANORlAIAAAAAAIDpSEoBAAAAAADAdCSlAAAAAAAAYDqSUgAAAAAAADAdSSkAAAAAAACYjqQUAAAAAAAATEdSCgAAAAAAAKYjKQXcIgYMGKCaNWve0LEvvviiLBZL4QYEXEPu9+7UqVOODgUAcJ0OHToki8WiOXPmWMuuZzxhsVj04osvFmpM7dq1U7t27Qr1nKBf4TgDBgxQmTJlHB0GihhJKaCIWSyWAm1r1651dKgOcTNdbAzD0Mcff6w2bdqoXLly8vDwUOPGjTVx4kRlZGQ4Orw8cv9xkN+WnJzs6BABACbo3r27PDw89Oeff+Zbp3///nJxcdHp06dNjOz67d69Wy+++KIOHTrk6FCs1q5da3N9dXV1la+vr9q1a6eXX35ZJ0+evOFzHz9+XC+++KK2bdtWeAHbcTP0q7Ozs3x8fNSrVy/t2bPnhs/78ssva/HixYUX6F9cunRJ77zzjlq0aKGyZcuqTJkyatGihd555x1dunSpSN7znxgwYEC+40Q3NzdHh4cSopSjAwBudR9//LHN67lz52rlypV5yuvXr/+P3ueDDz5QTk7ODR07ZswYjRo16h+9/60uOztb/fr104IFC3TPPffoxRdflIeHh7777jtNmDBB8fHxWrVqlXx9fR0dah7Tp0+3m/grV66c+cEAAEzXv39/ffXVV/riiy8UHh6eZ//58+e1ZMkSderUSRUrVrzh9zFjPLF7925NmDBB7dq1yzNDfMWKFUX63tcyYsQItWjRQtnZ2Tp58qS+//57jR8/XpMnT9aCBQvUoUOH6z7n8ePHNWHCBNWsWVPNmjUr/KD/383Qr5cuXdKOHTs0Y8YMrV27Vrt27ZKfn991n+/ll19Wr169FBYWVqhxZmRkqGvXrlq3bp3uu+8+DRgwQE5OTlq2bJmeeuopLVq0SEuXLpWnp2ehvu8/5erqqg8//DBPubOzswOiQUlEUgooYg8//LDN6x9++EErV67MU/5358+fl4eHR4Hfp3Tp0jcUnySVKlVKpUrx18HVvPbaa1qwYIGeffZZvf7669byIUOG6KGHHlJYWJgGDBigb7/91tS4CvI96dWrlypVqmRSRACA4qZ79+4qW7as5s2bZzcptWTJEmVkZKh///7/6H0cPZ5wcXFx2HtL0j333KNevXrZlG3fvl333nuvevbsqd27d6tKlSoOiu7GFbd+rVu3roYOHaq5c+fqP//5jwMjsxUVFaV169bp3Xff1fDhw63lQ4cO1dSpUzV8+HA9++yzmj59umkxGYahixcvyt3dPd86pUqVuua/S4CixO17QDHQrl07NWrUSElJSWrTpo08PDw0evRoSVcGil27dlXVqlXl6uqq2rVr66WXXlJ2drbNOf6+plTuWg9vvPGG3n//fdWuXVuurq5q0aKFNm/ebHOsvTUgLBaLhg8frsWLF6tRo0ZydXVVw4YNtWzZsjzxr127Vs2bN5ebm5tq166t9957r9DXqYqPj1dAQIDc3d1VqVIlPfzwwzp27JhNneTkZEVGRuq2226Tq6urqlSpovvvv99mKvqWLVsUGhqqSpUqyd3dXbVq1dKjjz561fe+cOGCXn/9df3rX/9STExMnv3dunVTRESEli1bph9++EGSdN999+n222+3e77g4GA1b97cpuyTTz6xtq9ChQrq06ePfv/9d5s6V/ue/BO50+Pj4uI0evRo+fn5ydPTU927d88Tg1Swz0KS9u7dq4ceekiVK1eWu7u76tatqxdeeCFPvbNnz2rAgAEqV66cvL29FRkZqfPnz9vUWblypVq3bq1y5cqpTJkyqlu3bqG0HQBKAnd3d/Xo0UMJCQlKTU3Ns3/evHkqW7asunfvrjNnzujZZ59V48aNVaZMGXl5ealz587avn37Nd/H3rU/MzNTzzzzjCpXrmx9j6NHj+Y59vDhw3riiSdUt25dubu7q2LFinrwwQdtruFz5szRgw8+KElq3759niUQ7K19lJqaqoEDB8rX11dubm5q2rSpPvroI5s61zNmul5NmzZVbGyszp49qylTptjsO3bsmB599FH5+vpax1mzZs2y7l+7dq1atGghSYqMjLS296/reP3444/q1KmTvL295eHhobZt22rjxo154jh27JgGDhxoHU/WqlVLQ4cOVVZW1k3Xr/fcc48k6ddff7Upf+ONN9SqVStVrFhR7u7uCggI0MKFC23qWCwWZWRk6KOPPrK2c8CAATb9dLXPJD9Hjx7VzJkz1aFDB5uEVK5hw4apffv2+vDDD63f/0aNGql9+/Z56ubk5KhatWo2ibicnBzFxsaqYcOGcnNzk6+vrx577DH98ccfNsfWrFlT9913n5YvX67mzZvL3d1d77333jXjv5Y5c+bIYrFo/fr1euyxx1SxYkV5eXkpPDw8TwySNG3aNDVs2FCurq6qWrWqhg0bprNnz+ap9+OPP6pLly4qX768PD091aRJE7399tt56h07dkxhYWEqU6aMKleurGeffTbPv4Xmz5+vgIAAlS1bVl5eXmrcuLHdc6H4YWoEUEycPn1anTt3Vp8+ffTwww9bbwObM2eOypQpo6ioKJUpU0arV6/WuHHjlJ6ebjNjJz/z5s3Tn3/+qccee0wWi0WvvfaaevTood9+++2as6s2bNigRYsW6YknnlDZsmX1zjvvqGfPnjpy5Ih1ev9PP/2kTp06qUqVKpowYYKys7M1ceJEVa5c+Z93yv+bM2eOIiMj1aJFC8XExCglJUVvv/22Nm7cqJ9++sl6G1rPnj31888/68knn1TNmjWVmpqqlStX6siRI9bX9957rypXrqxRo0apXLlyOnTokBYtWnTNfvjjjz/01FNP5fsLcHh4uGbPnq2vv/5aLVu2VO/evRUeHq7NmzdbB5TSlUH3Dz/8YPPZTZo0SWPHjtVDDz2kQYMG6eTJk3r33XfVpk0bm/ZJ+X9PrubMmTN5ykqVKpXn9r1JkybJYrHo+eefV2pqqmJjYxUSEqJt27ZZf2Er6GexY8cO3XPPPSpdurSGDBmimjVr6tdff9VXX32lSZMm2bzvQw89pFq1aikmJkZbt27Vhx9+KB8fH7366quSpJ9//ln33XefmjRpookTJ8rV1VUHDhywO+gGANjXv39/ffTRR1qwYIHNP5rPnDmj5cuXq2/fvnJ3d9fPP/+sxYsX68EHH1StWrWUkpKi9957T23bttXu3btVtWrV63rfQYMG6ZNPPlG/fv3UqlUrrV69Wl27ds1Tb/Pmzfr+++/Vp08f3XbbbTp06JCmT5+udu3aaffu3fLw8FCbNm00YsQIvfPOOxo9erR16YP8lkC4cOGC2rVrpwMHDmj48OGqVauW4uPjNWDAAJ09e1ZPPfWUTf1/Mma6ml69emngwIFasWKF9RqYkpKili1bWn8ErFy5sr799lsNHDhQ6enpevrpp1W/fn1NnDhR48aN05AhQ6zJmFatWkmSVq9erc6dOysgIEDjx4+Xk5OTZs+erQ4dOui7775TYGCgpCu3AAYGBurs2bMaMmSI6tWrp2PHjmnhwoU6f/78TdevuYnK8uXL25S//fbb6t69u/r376+srCzNnz9fDz74oL7++mvrd+7jjz/WoEGDFBgYqCFDhkiSateuXeDPJD/ffvutsrOz7c5EzBUeHq41a9Zo2bJlGjRokHr37q0XX3xRycnJNrchbtiwQcePH1efPn2sZY899ph1DDZixAgdPHhQU6ZM0U8//aSNGzfa9OO+ffvUt29fPfbYYxo8eLDq1q17zT6199AZFxcXeXl52ZQNHz5c5cqV04svvqh9+/Zp+vTpOnz4sPUHTulKcnrChAkKCQnR0KFDrfU2b95sE+vKlSt13333qUqVKnrqqafk5+enPXv26Ouvv7b5DmVnZys0NFRBQUF64403tGrVKr355puqXbu2hg4daj1X37591bFjR+v4cc+ePdq4cWOe7yOKIQOAqYYNG2b8/Y9e27ZtDUnGjBkz8tQ/f/58nrLHHnvM8PDwMC5evGgti4iIMGrUqGF9ffDgQUOSUbFiRePMmTPW8iVLlhiSjK+++spaNn78+DwxSTJcXFyMAwcOWMu2b99uSDLeffdda1m3bt0MDw8P49ixY9ay/fv3G6VKlcpzTnsiIiIMT0/PfPdnZWUZPj4+RqNGjYwLFy5Yy7/++mtDkjFu3DjDMAzjjz/+MCQZr7/+er7n+uKLLwxJxubNm68Z11/FxsYakowvvvgi3zpnzpwxJBk9evQwDMMw0tLSDFdXV2PkyJE29V577TXDYrEYhw8fNgzDMA4dOmQ4OzsbkyZNsqm3c+dOo1SpUjblV/ue2JP7udrb6tata623Zs0aQ5JRrVo1Iz093Vq+YMECQ5Lx9ttvG4ZR8M/CMAyjTZs2RtmyZa3tzJWTk5MnvkcffdSmzgMPPGBUrFjR+vqtt94yJBknT54sULsBAHldvnzZqFKlihEcHGxTPmPGDEOSsXz5csMwDOPixYtGdna2TZ2DBw8arq6uxsSJE23KJBmzZ8+2lv19PLFt2zZDkvHEE0/YnK9fv36GJGP8+PHWMnvjncTEREOSMXfuXGtZfHy8IclYs2ZNnvpt27Y12rZta32de/3+5JNPrGVZWVlGcHCwUaZMGes173rGTPbkXkfj4+PzrdO0aVOjfPny1tcDBw40qlSpYpw6dcqmXp8+fQxvb29rf2zevDlPPxvGlevpHXfcYYSGhtpcW8+fP2/UqlXL+Pe//20tCw8PN5ycnOyOf3KPLc79OmvWLOPkyZPG8ePHjWXLlhl16tQxLBaLsWnTJpv6f/8OZWVlGY0aNTI6dOhgU+7p6WlERETkeb+Cfib2PP3004Yk46effsq3ztatWw1JRlRUlGEYhrFv374842rDMIwnnnjCKFOmjPX9vvvuO0OS8emnn9rUW7ZsWZ7yGjVqGJKMZcuW5RvHX0VEROQ7VgwNDbXWmz17tiHJCAgIMLKysqzlr732miHJWLJkiWEYhpGammq4uLgY9957r83fI1OmTLF+loZx5e+jWrVqGTVq1DD++OMPm5j++n3Oje+vf/cYhmHceeedRkBAgPX1U089ZXh5eRmXL18uULtRvHD7HlBMuLq6KjIyMk/5X+8B//PPP3Xq1Cndc889On/+vPbu3XvN8/bu3dvml6TcX9l+++23ax4bEhJi/fVIkpo0aSIvLy/rsdnZ2Vq1apXCwsJsfjmtU6eOOnfufM3zF8SWLVuUmpqqJ554wuYpIF27dlW9evW0dOlSSVf6ycXFRWvXrrU7jVj638LeX3/99XU9ASX3aUVly5bNt07uvvT0dEmy3u6wYMECGYZhrRcXF6eWLVuqevXqkqRFixYpJydHDz30kE6dOmXd/Pz8dMcdd2jNmjU275Pf9+RqPv/8c61cudJmmz17dp564eHhNm3s1auXqlSpom+++UZSwT+LkydPav369Xr00Uet7cxl75bOxx9/3Ob1Pffco9OnT1v7MvdzW7JkyQ0v5g8AJZ2zs7P69OmjxMREm1vi5s2bJ19fX3Xs2FHSleuMk9OVfyJkZ2fr9OnT1tumt27del3vmXv9GDFihE25vRknfx3vXLp0SadPn1adOnVUrly5637fv76/n5+f+vbtay0rXbq0RowYoXPnzmndunU29f/JmOlaypQpYx1PGIahzz//XN26dZNhGDbX/9DQUKWlpV2zzdu2bdP+/fvVr18/nT592np8RkaGOnbsqPXr1ysnJ0c5OTlavHixunXrlmfpAMn+dflazO7XRx99VJUrV1bVqlXVqVMnpaWl6eOPP7aZiS7Zfof++OMPpaWl6Z577inQ9+effiY3Mlb817/+pWbNmikuLs5aJzs7WwsXLlS3bt2s7YmPj5e3t7f+/e9/28QVEBCgMmXK5Bkr1qpVS6Ghoddscy43N7c848SVK1fqlVdeyVN3yJAhNrOyhg4dqlKlSln/rK9atUpZWVl6+umnrX+PSNLgwYPl5eVlHSv+9NNPOnjwoJ5++uk8M/cLOlb86/enXLlyysjI0MqVKwvcbhQf3L4HFBPVqlWzu5Dkzz//rDFjxmj16tXWi1iutLS0a57370mB3EFBfombqx2be3zusampqbpw4YLq1KmTp569shtx+PBhSbI79bhevXrasGGDpCuD6FdffVUjR46Ur6+vWrZsqfvuu0/h4eHWKdFt27ZVz549NWHCBL311ltq166dwsLC1K9fP7m6uuYbQ+4g4mqP0rY3GOndu7cWL16sxMREtWrVSr/++quSkpIUGxtrrbN//34ZhqE77rjD7nn/Pq09v+/J1bRp06ZAC53/PQaLxaI6depY//FS0M8id5DQqFGjAsV3te+ol5eXevfurQ8//FCDBg3SqFGj1LFjR/Xo0UO9evWyGfAAAK6uf//+euuttzRv3jyNHj1aR48e1XfffacRI0ZYn7SVk5Ojt99+W9OmTdPBgwdt1m253ifzHT58WE5OTjY/cEn2ryMXLlxQTEyMZs+erWPHjtn8oFOQ8U5+73/HHXfkuVbk3paWe13L9U/GTNdy7tw56xjh5MmTOnv2rN5//329//77duvbW/vrr/bv3y9JioiIyLdOWlqasrKylJ6eXuBrckGY3a/jxo3TPffco3PnzumLL77Q/Pnz7V7/v/76a/33v//Vtm3blJmZaS0vSOLtn34m/2SsOHr0aB07dkzVqlXT2rVrlZqaqt69e1vr7N+/X2lpafLx8SlQXLVq1co3BnucnZ0VEhJSoLp/HyuWKVNGVapUueZY0cXFRbfffrt1f+56YAX5Xrq5ueVZFuSv/x6RpCeeeEILFixQ586dVa1aNd1777166KGH1KlTpwK1C45FUgooJuw9FePs2bNq27atvLy8NHHiRNWuXVtubm7aunWrnn/++QLNGsnvca5/HewVxbGO8PTTT6tbt25avHixli9frrFjxyomJkarV6/WnXfeKYvFooULF+qHH37QV199peXLl+vRRx/Vm2++qR9++EFlypSxe97cQdaOHTvyfXzwjh07JEkNGjSwlnXr1k0eHh5asGCBWrVqpQULFsjJycm6mKh0ZfBvsVj07bff2u3vv8d0taen3Kyu9T1zd3fX+vXrtWbNGi1dulTLli1TXFycOnTooBUrVvDIYgAooICAANWrV0+fffaZRo8erc8++0yGYdg8de/ll1/W2LFj9eijj+qll15ShQoV5OTkpKeffrpIZ6s++eSTmj17tp5++mkFBwfL29tbFotFffr0MW2WbFGNey5duqRffvnF+g/w3PY8/PDD+SaVmjRpctVz5p7j9ddfV7NmzezWKVOmjN11Jc32T/u1cePG1qRJWFiYzp8/r8GDB6t169by9/eXJH333Xfq3r272rRpo2nTpqlKlSoqXbq0Zs+erXnz5l3zPf7pZ/LXsWJ+n4e9sWLv3r0VHR2t+Ph4Pf3001qwYIG8vb1tkik5OTny8fHRp59+ave8f0/Y3GpjxYKM83x8fLRt2zYtX75c3377rb799lvNnj1b4eHheRbgR/FDUgooxtauXavTp09r0aJFatOmjbX84MGDDozqf3x8fOTm5qYDBw7k2Wev7EbUqFFD0pVFGzt06GCzb9++fdb9uWrXrq2RI0dq5MiR2r9/v5o1a6Y333xTn3zyibVOy5Yt1bJlS02aNEnz5s1T//79NX/+fA0aNMhuDLlPfZs3b55eeOEFuxfHuXPnSrry1L1cnp6euu+++xQfH6/JkycrLi5O99xzj82tjrVr15ZhGKpVq5b+9a9/XWfvFK7cX11zGYahAwcOWAdhBf0scp86uGvXrkKLzcnJSR07dlTHjh01efJkvfzyy3rhhRe0Zs2aAv+6BwC4Mltq7Nix2rFjh+bNm6c77rjD5jaohQsXqn379po5c6bNcWfPni3QrNu/qlGjhnJycvTrr7/azJzYt29fnroLFy5URESE3nzzTWvZxYsX8zyx63puN6tRo4Z27NihnJwcm5k1ucsf/H0MUVQWLlyoCxcuWG+pyn0SYXZ29jWvYfm1N3f2mZeX11XPUblyZXl5eV3zmnwz9esrr7yiL774QpMmTdKMGTMkXVmqwM3NTcuXL7eZ/W5vuQJ7bb2ez8Sezp07y9nZWR9//HG+i53PnTtXpUqVskk41apVS4GBgYqLi9Pw4cO1aNEihYWF2bShdu3aWrVqle6++26HJ5z2799v88TAc+fO6cSJE+rSpYsk27HiX59CnZWVpYMHD1r7Nvf7u2vXrkIbx7m4uKhbt27q1q2bcnJy9MQTT+i9997T2LFjC+0ODhQN7nsAirHc5Mdff0nKysrStGnTHBWSjdzpvosXL9bx48et5QcOHNC3335bKO/RvHlz+fj4aMaMGTZTsb/99lvt2bPH+jSV8+fP6+LFizbH1q5dW2XLlrUe98cff+T5VS7316y/nvvvPDw89Oyzz2rfvn164YUX8uxfunSp5syZo9DQULVs2dJmX+/evXX8+HF9+OGH2r59u810bEnq0aOHnJ2dNWHChDyxGYah06dP5xtXYZs7d67NtPOFCxfqxIkT1vXBCvpZVK5cWW3atNGsWbN05MgRm/e4kV+b7f3KW5DPDQCQV+6sqHHjxmnbtm02s6SkK9f2v/9dHR8fr2PHjl33e+VeP9555x2b8r/exn6193333XfzPPbd09NTkuw+Xv7vunTpouTkZJs1ey5fvqx3331XZcqUUdu2bQvSjH9k+/btevrpp1W+fHkNGzZM0pW29uzZU59//rndZNHJkyet/59fewMCAlS7dm298cYbOnfuXL7ncHJyUlhYmL766itt2bIlT73cPr+Z+rV27drq2bOn5syZo+TkZElX+tRisdh8Xw4dOqTFixfnOd7T0zNPO6/nM7HH399fkZGRWrVqlaZPn55n/4wZM7R69WoNHDhQt912m82+3r1764cfftCsWbN06tSpPGPFhx56SNnZ2XrppZfynPfy5csF+swKy/vvv2+zLuv06dN1+fJl65/1kJAQubi46J133rH58zxz5kylpaVZx4p33XWXatWqpdjY2Dzx38hY8e/jZScnJ+uPqowViz9mSgHFWKtWrVS+fHlFRERoxIgRslgs+vjjj4vV7XMvvviiVqxYobvvvltDhw5Vdna2pkyZokaNGmnbtm0FOselS5f03//+N095hQoV9MQTT+jVV19VZGSk2rZtq759+yolJUVvv/22atasqWeeeUaS9Msvv6hjx4566KGH1KBBA5UqVUpffPGFUlJSrI/U/eijjzRt2jQ98MADql27tv7880998MEH8vLysv7Ck59Ro0bpp59+0quvvqrExET17NlT7u7u2rBhgz755BPVr1/f7vTgLl26qGzZsnr22WetA56/ql27tv773/8qOjpahw4dUlhYmMqWLauDBw/qiy++0JAhQ/Tss88WqB/zs3DhQru3Jv773/+Wr6+v9XWFChXUunVrRUZGKiUlRbGxsapTp44GDx4s6cr6VgX5LKQr/wBp3bq17rrrLg0ZMkS1atXSoUOHtHTp0gJ/L3JNnDhR69evV9euXVWjRg2lpqZq2rRpuu2229S6desb6xQAKKFq1aqlVq1aacmSJZKUJyl13333aeLEiYqMjFSrVq20c+dOffrppzazHgqqWbNm6tu3r6ZNm6a0tDS1atVKCQkJdmdT33ffffr444/l7e2tBg0aKDExUatWrcqzjlWzZs3k7OysV199VWlpaXJ1dVWHDh3srrczZMgQvffeexowYICSkpJUs2ZNLVy4UBs3blRsbOxVF6W+Ed99950uXrxoXSB+48aN+vLLL+Xt7a0vvvjCusaldGW2z5o1axQUFKTBgwerQYMGOnPmjLZu3apVq1ZZf5CpXbu2ypUrpxkzZqhs2bLy9PRUUFCQatWqpQ8//FCdO3dWw4YNFRkZqWrVqunYsWNas2aNvLy89NVXX0m6ckvmihUr1LZtWw0ZMkT169fXiRMnFB8frw0bNqhcuXLFul/tee6557RgwQLFxsbqlVdeUdeuXTV58mR16tRJ/fr1U2pqqqZOnao6depYb5vLFRAQoFWrVmny5MmqWrWqatWqpaCgoAJ/Jvl56623tHfvXj3xxBNatmyZdUbU8uXLtWTJErVt29ZmJmCuhx56SM8++6yeffZZVahQIc/MobZt2+qxxx5TTEyMtm3bpnvvvVelS5fW/v37FR8fr7ffflu9evW64b68fPmyzR0Ff/XAAw9YE5bSlR/Hc8fb+/bt07Rp09S6dWt1795d0pUfJqOjozVhwgR16tRJ3bt3t9Zr0aKFHn74YUlXkkbTp09Xt27d1KxZM0VGRqpKlSrau3evfv75Zy1fvvy62jBo0CCdOXNGHTp00G233abDhw/r3XffVbNmzay3VqIYM+sxfwCuGDZsmPH3P3pt27Y1GjZsaLf+xo0bjZYtWxru7u5G1apVjf/85z/G8uXL8zy2NyIiwqhRo4b1de5jeF9//fU859TfHsP890c459YZNmxYnmNr1KiR5zG6CQkJxp133mm4uLgYtWvXNj788ENj5MiRhpubWz698D9XexRt7dq1rfXi4uKMO++803B1dTUqVKhg9O/f3zh69Kh1/6lTp4xhw4YZ9erVMzw9PQ1vb28jKCjIWLBggbXO1q1bjb59+xrVq1c3XF1dDR8fH+O+++4ztmzZcs04DcMwsrOzjdmzZxt333234eXlZbi5uRkNGzY0JkyYYJw7dy7f4/r3729IMkJCQvKt8/nnnxutW7c2PD09DU9PT6NevXrGsGHDjH379lnrXO17Yk/u55rflvv9yX3k8meffWZER0cbPj4+hru7u9G1a1fj8OHDec57rc8i165du4wHHnjAKFeunOHm5mbUrVvXGDt2bJ74Tp48aXNc7mOHDx48aBjGle/X/fffb1StWtVwcXExqlatavTt29f45ZdfCtwXAID/mTp1qiHJCAwMzLPv4sWLxsiRI40qVaoY7u7uxt13320kJiYabdu2Ndq2bWutlzvOmD17trXM3njiwoULxogRI4yKFSsanp6eRrdu3Yzff/89z1jkjz/+MCIjI41KlSoZZcqUMUJDQ429e/faHXd88MEHxu233244OzvbXM/+HqNhGEZKSor1vC4uLkbjxo1tYv5rWwoyZrIn9zqau5UuXdqoXLmy0aZNG2PSpElGamqq3eNSUlKMYcOGGf7+/kbp0qUNPz8/o2PHjsb7779vU2/JkiVGgwYNjFKlSuXp859++sno0aOHUbFiRcPV1dWoUaOG8dBDDxkJCQk25zh8+LARHh5uVK5c2XB1dTVuv/12Y9iwYUZmZmax79f4+Hi7+9u1a2d4eXkZZ8+eNQzDMGbOnGnccccdhqurq1GvXj1j9uzZdr+Te/fuNdq0aWO4u7sbkmy+XwX9TPKTmZlpvPXWW0ZAQIDh6elpeHh4GHfddZcRGxtrZGVl5Xvc3XffbUgyBg0alG+d999/3wgICDDc3d2NsmXLGo0bNzb+85//GMePH7fWqVGjhtG1a9cCxWoYVx+H/3Usljs2W7dunTFkyBCjfPnyRpkyZYz+/fsbp0+fznPeKVOmGPXq1TNKly5t+Pr6GkOHDjX++OOPPPU2bNhg/Pvf/zbKli1reHp6Gk2aNDHeffddm/g8PT3zHPf3z3XhwoXGvffea/j4+BguLi5G9erVjccee8w4ceJEgfsCjmMxjGI05QLALSMsLEw///xznnWKUPysXbtW7du3V3x8/D/6pQ0AAAC3njlz5igyMlKbN29W8+bNHR0ObjGsKQXgH7tw4YLN6/379+ubb75Ru3btHBMQAAAAAKDYY00pAP/Y7bffrgEDBuj222/X4cOHNX36dLm4uOg///mPo0MDAAAAABRTJKUA/GOdOnXSZ599puTkZLm6uio4OFgvv/yy7rjjDkeHBgAAAAAoplhTCgAAAAAAAKZjTSkAAAAAAACYjqQUAAAAAAAATMeaUnbk5OTo+PHjKlu2rCwWi6PDAQAAJjAMQ3/++aeqVq0qJyd+tyssjKsAACh5CjquIillx/Hjx+Xv7+/oMAAAgAP8/vvvuu222xwdxi2DcRUAACXXtcZVJKXsKFu2rKQrnefl5eXgaAAAgBnS09Pl7+9vHQegcDCuAgCg5CnouIqklB25U8u9vLwYPAEAUMJwi1nhYlwFAEDJda1xFQsmAAAAAAAAwHQkpQAAAAAAAGA6klIAAAAAAAAwHUkpAAAAAAAAmI6kFAAAAAAAAExHUgoAAAAAAACmIykFAAAAAAAA05GUAgAAAAAAgOlISgEAAAAAAMB0JKUAAAAAAABgOpJSAAAAAAAAMB1JKQAAAAAAAJiOpBQAAAAAAABMR1IKAAAAAAAApiMpBQAAAAAAANORlAIAAAAAAIDpikVSaurUqapZs6bc3NwUFBSkTZs2XbV+bGys6tatK3d3d/n7++uZZ57RxYsXrfunT5+uJk2ayMvLS15eXgoODta3335b1M0AAAAAAABAATk8KRUXF6eoqCiNHz9eW7duVdOmTRUaGqrU1FS79efNm6dRo0Zp/Pjx2rNnj2bOnKm4uDiNHj3aWue2227TK6+8oqSkJG3ZskUdOnTQ/fffr59//tmsZgEAAAAAAOAqLIZhGI4MICgoSC1atNCUKVMkSTk5OfL399eTTz6pUaNG5ak/fPhw7dmzRwkJCdaykSNH6scff9SGDRvyfZ8KFSro9ddf18CBA68ZU3p6ury9vZWWliYvL68baBUAALjZcP0vGvQrAAAlT0Gv/w6dKZWVlaWkpCSFhIRYy5ycnBQSEqLExES7x7Rq1UpJSUnWW/x+++03ffPNN+rSpYvd+tnZ2Zo/f74yMjIUHBxc+I0AAAAAAADAdSvlyDc/deqUsrOz5evra1Pu6+urvXv32j2mX79+OnXqlFq3bi3DMHT58mU9/vjjNrfvSdLOnTsVHBysixcvqkyZMvriiy/UoEEDu+fMzMxUZmam9XV6evo/bBkAAAAAAACuxuFrSl2vtWvX6uWXX9a0adO0detWLVq0SEuXLtVLL71kU69u3bratm2bfvzxRw0dOlQRERHavXu33XPGxMTI29vbuvn7+5vRFAAAAAAAgBLLoWtKZWVlycPDQwsXLlRYWJi1PCIiQmfPntWSJUvyHHPPPfeoZcuWev31161ln3zyiYYMGaJz587Jycl+ni0kJES1a9fWe++9l2efvZlS/v7+rH0AAEAJwtpHRYN+BQCg5Lkp1pRycXFRQECAzaLlOTk5SkhIyHf9p/Pnz+dJPDk7O0uSrpZfy8nJsUk8/ZWrq6u8vLxsNgAAAAAAABQdh64pJUlRUVGKiIhQ8+bNFRgYqNjYWGVkZCgyMlKSFB4ermrVqikmJkaS1K1bN02ePFl33nmngoKCdODAAY0dO1bdunWzJqeio6PVuXNnVa9eXX/++afmzZuntWvXavny5Q5rJwAAAAAAAP7H4Ump3r176+TJkxo3bpySk5PVrFkzLVu2zLr4+ZEjR2xmRo0ZM0YWi0VjxozRsWPHVLlyZXXr1k2TJk2y1klNTVV4eLhOnDghb29vNWnSRMuXL9e///1v09sHAAAAAACAvBy6plRxxdoHAACUPFz/iwb9CgBAyXNTrCkFAAAAAACAkomkFAAAAAAAAExHUgoAAAAAAACmIykFAAAAAAAA05GUAgAAAAAAgOlISgEAAAAAAMB0JKUAAAAAAABgOpJSAAAAAAAAMB1JKQAAAAAAAJiOpBQAAAAAAABMR1IKAAAAAAAApiMpBQAAAAAAANORlAIAAAAAAIDpSEoBAAAAAADAdCSlAAAAAAAAYDqSUgAAAAAAADAdSSkAAAAAAACYjqQUAAAAAAAATEdSCgAAAAAAAKYjKQUAAAAAAADTkZQCAAAAAACA6UhKAQAA3EKmTp2qmjVrys3NTUFBQdq0adNV68fHx6tevXpyc3NT48aN9c033+Rb9/HHH5fFYlFsbGwhRw0AAEoiklIAAAC3iLi4OEVFRWn8+PHaunWrmjZtqtDQUKWmptqt//3336tv374aOHCgfvrpJ4WFhSksLEy7du3KU/eLL77QDz/8oKpVqxZ1MwAAQAlBUgoAAOAWMXnyZA0ePFiRkZFq0KCBZsyYIQ8PD82aNctu/bfffludOnXSc889p/r16+ull17SXXfdpSlTptjUO3bsmJ588kl9+umnKl26tBlNAQAAJQBJKQAAgFtAVlaWkpKSFBISYi1zcnJSSEiIEhMT7R6TmJhoU1+SQkNDbern5OTokUce0XPPPaeGDRteM47MzEylp6fbbAAAAPaQlAIAALgFnDp1StnZ2fL19bUp9/X1VXJyst1jkpOTr1n/1VdfValSpTRixIgCxRETEyNvb2/r5u/vf50tAQAAJQVJKQAAANiVlJSkt99+W3PmzJHFYinQMdHR0UpLS7Nuv//+exFHCQAAblYkpQAAAG4BlSpVkrOzs1JSUmzKU1JS5OfnZ/cYPz+/q9b/7rvvlJqaqurVq6tUqVIqVaqUDh8+rJEjR6pmzZp2z+nq6iovLy+bDQAAwB6SUgAAALcAFxcXBQQEKCEhwVqWk5OjhIQEBQcH2z0mODjYpr4krVy50lr/kUce0Y4dO7Rt2zbrVrVqVT333HNavnx50TUGAACUCKUcHQAAAAAKR1RUlCIiItS8eXMFBgYqNjZWGRkZioyMlCSFh4erWrVqiomJkSQ99dRTatu2rd5880117dpV8+fP15YtW/T+++9LkipWrKiKFSvavEfp0qXl5+enunXrmts4AABwyyEpBQAAcIvo3bu3Tp48qXHjxik5OVnNmjXTsmXLrIuZHzlyRE5O/5so36pVK82bN09jxozR6NGjdccdd2jx4sVq1KiRo5oAAABKEIthGIajgyhu0tPT5e3trbS0NNZBAACghOD6XzToVwAASp6CXv9ZUwoAAAAAAACmIykFAAAAAAAA05GUAgAAAAAAgOlISgEAAAAAAMB0xSIpNXXqVNWsWVNubm4KCgrSpk2brlo/NjZWdevWlbu7u/z9/fXMM8/o4sWL1v0xMTFq0aKFypYtKx8fH4WFhWnfvn1F3QwAAAAAAAAUkMOTUnFxcYqKitL48eO1detWNW3aVKGhoUpNTbVbf968eRo1apTGjx+vPXv2aObMmYqLi9Po0aOtddatW6dhw4bphx9+0MqVK3Xp0iXde++9ysjIMKtZAAAAAAAAuAqLYRiGIwMICgpSixYtNGXKFElSTk6O/P399eSTT2rUqFF56g8fPlx79uxRQkKCtWzkyJH68ccftWHDBrvvcfLkSfn4+GjdunVq06bNNWPi0cUAAJQ8XP+LBv0KAEDJU9Drv0NnSmVlZSkpKUkhISHWMicnJ4WEhCgxMdHuMa1atVJSUpL1Fr/ffvtN33zzjbp06ZLv+6SlpUmSKlSoYHd/Zmam0tPTbTYAAAAAAAAUnVKOfPNTp04pOztbvr6+NuW+vr7au3ev3WP69eunU6dOqXXr1jIMQ5cvX9bjjz9uc/veX+Xk5Ojpp5/W3XffrUaNGtmtExMTowkTJvyzxgAAAAAAAKDAHL6m1PVau3atXn75ZU2bNk1bt27VokWLtHTpUr300kt26w8bNky7du3S/Pnz8z1ndHS00tLSrNvvv/9eVOEDAAAAAABADp4pValSJTk7OyslJcWmPCUlRX5+fnaPGTt2rB555BENGjRIktS4cWNlZGRoyJAheuGFF+Tk9L882/Dhw/X1119r/fr1uu222/KNw9XVVa6uroXQIgAAAAAAABSEQ2dKubi4KCAgwGbR8pycHCUkJCg4ONjuMefPn7dJPEmSs7OzJCl3zXbDMDR8+HB98cUXWr16tWrVqlVELQAAAAAAAMCNcOhMKUmKiopSRESEmjdvrsDAQMXGxiojI0ORkZGSpPDwcFWrVk0xMTGSpG7dumny5Mm68847FRQUpAMHDmjs2LHq1q2bNTk1bNgwzZs3T0uWLFHZsmWVnJwsSfL29pa7u7tjGgoAAAAAAAArhyelevfurZMnT2rcuHFKTk5Ws2bNtGzZMuvi50eOHLGZGTVmzBhZLBaNGTNGx44dU+XKldWtWzdNmjTJWmf69OmSpHbt2tm81+zZszVgwIAibxMAAAAAAACuzmLk3vMGq/T0dHl7eystLU1eXl6ODgcAAJiA63/RoF8BACh5Cnr9v+mevgcAAAAAAICbH0kpAAAAAAAAmI6kFAAAAAAAAExHUgoAAAAAAACmIykFAAAAAAAA05GUAgAAAAAAgOlISgEAAAAAAMB0JKUAAAAAAABgOpJSAAAAAAAAMB1JKQAAAAAAAJiOpBQAAAAAAABMR1IKAAAAAAAApiMpBQAAAAAAANORlAIAAAAAAIDpSEoBAAAAAADAdCSlAAAAAAAAYDqSUgAAAAAAADAdSSkAAAAAAACYjqQUAAAAAAAATEdSCgAAAAAAAKYjKQUAAAAAAADTkZQCAAAAAACA6UhKAQAAAAAAwHQkpQAAAAAAAGA6klIAAAAAAAAwHUkpAAAAAAAAmI6kFAAAAAAAAExHUgoAAAAAAACmIykFAAAAAAAA05GUAgAAAAAAgOlISgEAAAAAAMB0JKUAAAAAAABgOpJSAAAAAAAAMB1JKQAAAAAAAJiOpBQAAAAAAABM5/Ck1NSpU1WzZk25ubkpKChImzZtumr92NhY1a1bV+7u7vL399czzzyjixcvWvevX79e3bp1U9WqVWWxWLR48eIibgEAAAAAAACul0OTUnFxcYqKitL48eO1detWNW3aVKGhoUpNTbVbf968eRo1apTGjx+vPXv2aObMmYqLi9Po0aOtdTIyMtS0aVNNnTrVrGYAAAAAAADgOpVy5JtPnjxZgwcPVmRkpCRpxowZWrp0qWbNmqVRo0blqf/999/r7rvvVr9+/SRJNWvWVN++ffXjjz9a63Tu3FmdO3c2pwEAAAAAAAC4IQ6bKZWVlaWkpCSFhIT8LxgnJ4WEhCgxMdHuMa1atVJSUpL1Fr/ffvtN33zzjbp06WJKzAAAAAAAACgcDpspderUKWVnZ8vX19em3NfXV3v37rV7TL9+/XTq1Cm1bt1ahmHo8uXLevzxx21u37sRmZmZyszMtL5OT0//R+cDAAAAAADA1Tl8ofPrsXbtWr388suaNm2atm7dqkWLFmnp0qV66aWX/tF5Y2Ji5O3tbd38/f0LKWIAAAAAAADY47CZUpUqVZKzs7NSUlJsylNSUuTn52f3mLFjx+qRRx7RoEGDJEmNGzdWRkaGhgwZohdeeEFOTjeWY4uOjlZUVJT1dXp6OokpAAAAAACAIuSwmVIuLi4KCAhQQkKCtSwnJ0cJCQkKDg62e8z58+fzJJ6cnZ0lSYZh3HAsrq6u8vLystkAAAAAAABQdBz69L2oqChFRESoefPmCgwMVGxsrDIyMqxP4wsPD1e1atUUExMjSerWrZsmT56sO++8U0FBQTpw4IDGjh2rbt26WZNT586d04EDB6zvcfDgQW3btk0VKlRQ9erVzW8kAAAAAAAA8nBoUqp37946efKkxo0bp+TkZDVr1kzLli2zLn5+5MgRm5lRY8aMkcVi0ZgxY3Ts2DFVrlxZ3bp106RJk6x1tmzZovbt21tf596WFxERoTlz5pjTMAAAAAAAAFyVxfgn973dotLT0+Xt7a20tDRu5QMAoITg+l806FcAAEqegl7/b6qn7wEAAAAAAODWQFIKAAAAAAAApiMpBQAAcAuZOnWqatasKTc3NwUFBWnTpk1XrR8fH6969erJzc1NjRs31jfffGPdd+nSJT3//PNq3LixPD09VbVqVYWHh+v48eNF3QwAAFACkJQCAAC4RcTFxSkqKkrjx4/X1q1b1bRpU4WGhio1NdVu/e+//159+/bVwIED9dNPPyksLExhYWHatWuXJOn8+fPaunWrxo4dq61bt2rRokXat2+funfvbmazAADALYqFzu1gQU4AAEqeW+H6HxQUpBYtWmjKlCmSpJycHPn7++vJJ5/UqFGj8tTv3bu3MjIy9PXXX1vLWrZsqWbNmmnGjBl232Pz5s0KDAzU4cOHVb169WvGdCv0KwAAuD4sdA4AAFCCZGVlKSkpSSEhIdYyJycnhYSEKDEx0e4xiYmJNvUlKTQ0NN/6kpSWliaLxaJy5crZ3Z+Zman09HSbDQAAwB6SUgAAALeAU6dOKTs7W76+vjblvr6+Sk5OtntMcnLyddW/ePGinn/+efXt2zffXz1jYmLk7e1t3fz9/W+gNQAAoCQgKQUAAIBrunTpkh566CEZhqHp06fnWy86OlppaWnW7ffffzcxSgAAcDMp5egAAAAA8M9VqlRJzs7OSklJsSlPSUmRn5+f3WP8/PwKVD83IXX48GGtXr36qmtDuLq6ytXV9QZbAQAAShJmSgEAANwCXFxcFBAQoISEBGtZTk6OEhISFBwcbPeY4OBgm/qStHLlSpv6uQmp/fv3a9WqVapYsWLRNAAAAJQ4zJQCAAC4RURFRSkiIkLNmzdXYGCgYmNjlZGRocjISElSeHi4qlWrppiYGEnSU089pbZt2+rNN99U165dNX/+fG3ZskXvv/++pCsJqV69emnr1q36+uuvlZ2dbV1vqkKFCnJxcXFMQwEAwC2BpBQAAMAtonfv3jp58qTGjRun5ORkNWvWTMuWLbMuZn7kyBE5Of1vonyrVq00b948jRkzRqNHj9Ydd9yhxYsXq1GjRpKkY8eO6csvv5QkNWvWzOa91qxZo3bt2pnSLgAAcGuyGIZhODqI4iY9PV3e3t5KS0u76poJAADg1sH1v2jQrwAAlDwFvf6zphQAAAAAAABMR1IKAAAAAAAApiMpBQAAAAAAANORlAIAAAAAAIDpSEoBAAAAAADAdCSlAAAAAAAAYDqSUgAAAAAAADAdSSkAAAAAAACYjqQUAAAAAAAATEdSCgAAAAAAAKYjKQUAAAAAAADTkZQCAAAAAACA6UhKAQAAAAAAwHQkpQAAAAAAAGA6klIAAAAAAAAwHUkpAACAIpCVlaV9+/bp8uXLjg4FAACgWCIpBQAAUIjOnz+vgQMHysPDQw0bNtSRI0ckSU8++aReeeUVB0cHAABQfJCUAgAAKETR0dHavn271q5dKzc3N2t5SEiI4uLiHBgZAABA8VLK0QEAAADcShYvXqy4uDi1bNlSFovFWt6wYUP9+uuvDowMAACgeGGmFAAAQCE6efKkfHx88pRnZGTYJKkAAABKOpJSAAAAhah58+ZaunSp9XVuIurDDz9UcHCwo8ICAAAodrh9DwAAoBC9/PLL6ty5s3bv3q3Lly/r7bff1u7du/X9999r3bp1jg4PAACg2GCmFAAAQCFq3bq1tm3bpsuXL6tx48ZasWKFfHx8lJiYqICAAEeHBwAAUGwUi6TU1KlTVbNmTbm5uSkoKEibNm26av3Y2FjVrVtX7u7u8vf31zPPPKOLFy/+o3MCAAAUltq1a+uDDz7Qpk2btHv3bn3yySdq3Lixo8MCAAAoVhyelIqLi1NUVJTGjx+vrVu3qmnTpgoNDVVqaqrd+vPmzdOoUaM0fvx47dmzRzNnzlRcXJxGjx59w+cEAAAoLM7OznbHHKdPn5azs7MDIgIAACieHJ6Umjx5sgYPHqzIyEg1aNBAM2bMkIeHh2bNmmW3/vfff6+7775b/fr1U82aNXXvvfeqb9++NjOhrvecAAAAhcUwDLvlmZmZcnFxMTkaAACA4suhC51nZWUpKSlJ0dHR1jInJyeFhIQoMTHR7jGtWrXSJ598ok2bNikwMFC//fabvvnmGz3yyCM3fM7MzExlZmZaX6enpxdG8wAAQAnyzjvvSLrytL0PP/xQZcqUse7Lzs7W+vXrVa9ePUeFBwAAUOw4NCl16tQpZWdny9fX16bc19dXe/futXtMv379dOrUKbVu3VqGYejy5ct6/PHHrbfv3cg5Y2JiNGHChEJoEQAAKKneeustSVdmSs2YMcPmVj0XFxfVrFlTM2bMcFR4AAAAxY5Dk1I3Yu3atXr55Zc1bdo0BQUF6cCBA3rqqaf00ksvaezYsTd0zujoaEVFRVlfp6eny9/fv7BCBgAAJcDBgwclSe3bt9eiRYtUvnx5B0cEAABQvDk0KVWpUiU5OzsrJSXFpjwlJUV+fn52jxk7dqweeeQRDRo0SJLUuHFjZWRkaMiQIXrhhRdu6Jyurq5ydXUthBYBAICSbs2aNY4OAQAA4Kbg0KSUi4uLAgIClJCQoLCwMElSTk6OEhISNHz4cLvHnD9/Xk5Otuuz506PNwzjhs4JAABQmI4ePaovv/xSR44cUVZWls2+yZMnOygqAACA4sXht+9FRUUpIiJCzZs3V2BgoGJjY5WRkaHIyEhJUnh4uKpVq6aYmBhJUrdu3TR58mTdeeed1tv3xo4dq27dulmTU9c6JwAAQFFJSEhQ9+7ddfvtt2vv3r1q1KiRDh06JMMwdNdddzk6PAAAgGLD4Ump3r176+TJkxo3bpySk5PVrFkzLVu2zLpQ+ZEjR2xmRo0ZM0YWi0VjxozRsWPHVLlyZXXr1k2TJk0q8DkBAACKSnR0tJ599llNmDBBZcuW1eeffy4fHx/1799fnTp1cnR4AAAAxYbFMAzD0UEUN+np6fL29lZaWpq8vLwcHQ4AADBBYV3/y5Ytq23btql27doqX768NmzYoIYNG2r79u26//77dejQocIL+ibAuAoAgJKnoNd/p3z3AAAA4Lp5enpa15GqUqWKfv31V+u+U6dOOSosAACAYsfht+8BAADcSlq2bKkNGzaofv366tKli0aOHKmdO3dq0aJFatmypaPDAwAAKDZISgEAABSiyZMn69y5c5KkCRMm6Ny5c4qLi9Mdd9zBk/cAAAD+gqQUAABAIbr99tut/+/p6akZM2Y4MBoAAIDiizWlAAAATLBo0SI1adLE0WEAAAAUGySlAAAACsl7772nXr16qV+/fvrxxx8lSatXr9add96pRx55RHfffbeDIwQAACg+SEoBAAAUgldeeUVPPvmkDh06pC+//FIdOnTQyy+/rP79+6t37946evSopk+f7ugwAQAAig3WlAIAACgEs2fP1gcffKCIiAh99913atu2rb7//nsdOHBAnp6ejg4PAACg2GGmFAAAQCE4cuSIOnToIEm65557VLp0aU2YMIGEFAAAQD5ISgEAABSCzMxMubm5WV+7uLioQoUKDowIAACgeOP2PQAAgEIyduxYeXh4SJKysrL03//+V97e3jZ1Jk+e7IjQAAAAih2SUgAAAIWgTZs22rdvn/V1q1at9Ntvv9nUsVgsZocFAABQbJGUAgAAKARr1651dAgAAAA3FdaUAgAAAAAAgOlISgEAAAAAAMB0JKUAAAAAAABgOpJSAAAAAAAAMB1JKQAAAAAAAJiOp+8BAAAUsrNnz2rTpk1KTU1VTk6Ozb7w8HAHRQUAAFC8kJQCAAAoRF999ZX69++vc+fOycvLSxaLxbrPYrGQlAIAAPh/3L4HAABQiEaOHKlHH31U586d09mzZ/XHH39YtzNnzjg6PAAAgGLjhpJSv//+u44ePWp9vWnTJj399NN6//33Cy0wAACAm9GxY8c0YsQIeXh4ODoUAACAYu2GklL9+vXTmjVrJEnJycn697//rU2bNumFF17QxIkTCzVAAACAm0loaKi2bNni6DAAAACKvRtaU2rXrl0KDAyUJC1YsECNGjXSxo0btWLFCj3++OMaN25coQYJAABws+jatauee+457d69W40bN1bp0qVt9nfv3t1BkQEAABQvN5SUunTpklxdXSVJq1atsg6u6tWrpxMnThRedAAAADeZwYMHS5Ld2eMWi0XZ2dlmhwQAAFAs3dDtew0bNtSMGTP03XffaeXKlerUqZMk6fjx46pYsWKhBggAAHAzycnJyXcjIQUAAPA/N5SUevXVV/Xee++pXbt26tu3r5o2bSpJ+vLLL6239QEAAAAAAAD5uaHb99q1a6dTp04pPT1d5cuXt5YPGTKEJ80AAIASb926dXrjjTe0Z88eSVKDBg303HPP6Z577nFwZAAAAMXHDc2UunDhgjIzM60JqcOHDys2Nlb79u2Tj49PoQYIAABwM/nkk08UEhIiDw8PjRgxQiNGjJC7u7s6duyoefPmFfn7T506VTVr1pSbm5uCgoK0adOmq9aPj49XvXr15ObmpsaNG+ubb76x2W8YhsaNG6cqVarI3d1dISEh2r9/f1E2AQAAlBA3lJS6//77NXfuXEnS2bNnFRQUpDfffFNhYWGaPn16oQYIAABwM5k0aZJee+01xcXFWZNScXFxeuWVV/TSSy8V6XvHxcUpKipK48eP19atW9W0aVOFhoYqNTXVbv3vv/9effv21cCBA/XTTz8pLCxMYWFh2rVrl7XOa6+9pnfeeUczZszQjz/+KE9PT4WGhurixYtF2hYAAHDrsxiGYVzvQZUqVdK6devUsGFDffjhh3r33Xf1008/6fPPP9e4ceOsU9VvVunp6fL29lZaWpq8vLwcHQ4AADBBYV3/XV1d9fPPP6tOnTo25QcOHFCjRo2KNJkTFBSkFi1aaMqUKZKuLLru7++vJ598UqNGjcpTv3fv3srIyNDXX39tLWvZsqWaNWumGTNmyDAMVa1aVSNHjtSzzz4rSUpLS5Ovr6/mzJmjPn36XDMmxlUAAJQ8Bb3+39BMqfPnz6ts2bKSpBUrVqhHjx5ycnJSy5Ytdfjw4RuLGAAA4Bbg7++vhISEPOWrVq2Sv79/kb1vVlaWkpKSFBISYi1zcnJSSEiIEhMT7R6TmJhoU1+SQkNDrfUPHjyo5ORkmzre3t4KCgrK95wAAAAFdUMLndepU0eLFy/WAw88oOXLl+uZZ56RJKWmpvILGAAAKNFGjhypESNGaNu2bWrVqpUkaePGjZozZ47efvvtInvfU6dOKTs7W76+vjblvr6+2rt3r91jkpOT7dZPTk627s8ty6/O32VmZiozM9P6Oj09/foaAgAASowbSkqNGzdO/fr10zPPPKMOHTooODhY0pVZU3feeWehBggAAHAzGTp0qPz8/PTmm29qwYIFkqT69esrLi5O999/v4OjK3oxMTGaMGGCo8MAAAA3gRtKSvXq1UutW7fWiRMn1LRpU2t5x44d9cADDxRacAAAADejBx54wPQxUaVKleTs7KyUlBSb8pSUFPn5+dk9xs/P76r1c/+bkpKiKlWq2NRp1qyZ3XNGR0crKirK+jo9Pb1Ib1sEAAA3rxtaU0q6Mki58847dfz4cR09elSSFBgYqHr16l33ua7n0cXt2rWTxWLJs3Xt2tVaJyUlRQMGDFDVqlXl4eGhTp068ehiAABwS3NxcVFAQIDNelY5OTlKSEiwzmr/u+Dg4DzrX61cudJav1atWvLz87Opk56erh9//DHfc7q6usrLy8tmAwAAsOeGklI5OTmaOHGivL29VaNGDdWoUUPlypXTSy+9pJycnOs61/U+unjRokU6ceKEddu1a5ecnZ314IMPSpIMw1BYWJh+++03LVmyRD/99JNq1KihkJAQZWRk3EhzAQAArqpChQo6deqUJKl8+fKqUKFCvltRioqK0gcffKCPPvpIe/bs0dChQ5WRkaHIyEhJUnh4uKKjo631n3rqKS1btkxvvvmm9u7dqxdffFFbtmzR8OHDJUkWi0VPP/20/vvf/+rLL7/Uzp07FR4erqpVqyosLKxI2wIAAG59N3T73gsvvKCZM2fqlVde0d133y1J2rBhg1588UVdvHhRkyZNKvC5Jk+erMGDB1sHSzNmzNDSpUs1a9Ysu48u/vtgbv78+fLw8LAmpfbv368ffvhBu3btUsOGDSVJ06dPl5+fnz777DMNGjToRpoMAACQr7feesv6ZOK33npLFovFIXH07t1bJ0+e1Lhx45ScnKxmzZpp2bJl1oXKjxw5Iien//0m2apVK82bN09jxozR6NGjdccdd2jx4sVq1KiRtc5//vMfZWRkaMiQITp79qxat26tZcuWyc3NzfT2AQCAW4vFMAzjeg+qWrWqZsyYoe7du9uUL1myRE888YSOHTtWoPNkZWXJw8NDCxcutPm1LSIiQmfPntWSJUuueY7GjRsrODhY77//viRp586datKkiQ4cOKDatWtb6/n7+6tjx46aM2fONc+Znp4ub29vpaWlMeUcAIASgut/0aBfAQAoeQp6/b+h2/fOnDljd+2oevXq6cyZMwU+z9UeXZzfY4b/atOmTdq1a5fN7Kd69eqpevXqio6O1h9//KGsrCy9+uqrOnr0qE6cOGH3PJmZmUpPT7fZAAAAboSzs7PdZQhOnz4tZ2dnB0QEAABQPN1QUqpp06aaMmVKnvIpU6aoSZMm/ziogpo5c6YaN26swMBAa1np0qW1aNEi/fLLL6pQoYI8PDy0Zs0ade7c2Wa6+l/FxMTI29vbuvGEGAAAcKPym4SemZkpFxcXk6MBAAAovm5oTanXXntNXbt21apVq6xPXklMTNTvv/+ub775psDnuZFHF+fKyMjQ/PnzNXHixDz7AgICtG3bNqWlpSkrK0uVK1dWUFCQmjdvbvdcPLoYAAD8U++8846kK4uDf/jhhypTpox1X3Z2ttavX39DTykGAAC4Vd1QUqpt27b65ZdfNHXqVO3du1eS1KNHDw0ZMkT//e9/dc899xToPH99dHHumlK5jy7OfepLfuLj45WZmamHH3443zre3t6Srix+vmXLFr300kt267m6usrV1bVAMQMAANjz1ltvSboyU2rGjBk2t+q5uLioZs2amjFjhqPCAwAAKHZuaKHz/Gzfvl133XWXsrOzC3xMXFycIiIi9N577ykwMFCxsbFasGCB9u7dK19fX4WHh6tatWqKiYmxOe6ee+5RtWrVNH/+/DznjI+PV+XKlVW9enXt3LlTTz31lAICAvT5558XKCYW5AQAoOQprOt/+/bttWjRIpUvX74Qo7t5Ma4CAKDkKej1/4ZmShWm6310sSTt27dPGzZs0IoVK+ye88SJE4qKilJKSoqqVKmi8PBwjR07tsjbAgAAsGbNGkeHAAAAcFNw+Eyp4ohf9AAAKHkK6/rfs2dPBQYG6vnnn7cpf+2117R582bFx8f/01BvKoyrAAAoeQp6/b+hp+8BAADAvvXr16tLly55yjt37qz169c7ICIAAIDi6bpu3+vRo8dV9589e/afxAIAAHDTO3funFxcXPKUly5dWunp6Q6ICAAAoHi6rplS3t7eV91q1Kih8PDwoooVAACg2GvcuLHi4uLylM+fP18NGjRwQEQAAADF03XNlJo9e3ZRxQEAAHBLGDt2rHr06KFff/1VHTp0kCQlJCTos88+K3HrSQEAAFyNw5++BwAAcCvp1q2bFi9erJdfflkLFy6Uu7u7mjRpolWrVqlt27aODg8AAKDYICkFAABQyLp27aquXbs6OgwAAIBijafvAQAAFLKzZ8/qww8/1OjRo3XmzBlJ0tatW3Xs2DEHRwYAAFB8MFMKAACgEO3YsUMhISHy9vbWoUOHNGjQIFWoUEGLFi3SkSNHNHfuXEeHCAAAUCwwUwoAAKAQRUVFacCAAdq/f7/c3Nys5V26dNH69esdGBkAAEDxQlIKAACgEG3evFmPPfZYnvJq1aopOTnZAREBAAAUTySlAAAACpGrq6vS09PzlP/yyy+qXLmyAyICAAAonkhKAQAAFKLu3btr4sSJunTpkiTJYrHoyJEjev7559WzZ08HRwcAAFB8kJQCAAAoRG+++abOnTsnHx8fXbhwQW3btlWdOnVUtmxZTZo0ydHhAQAAFBs8fQ8AAKAQeXt7a+XKldq4caO2b9+uc+fO6a677lJISIijQwMAAChWSEoBAAAUorlz56p37966++67dffdd1vLs7KyNH/+fIWHhzswOgAAgOKD2/cAAAAKUWRkpNLS0vKU//nnn4qMjHRARAAAAMUTSSkAAIBCZBiGLBZLnvKjR4/K29vbAREBAAAUT9y+BwAAUAjuvPNOWSwWWSwWdezYUaVK/W+YlZ2drYMHD6pTp04OjBAAAKB4ISkFAABQCMLCwiRJ27ZtU2hoqMqUKWPd5+Liopo1a6pnz54Oig4AAKD4ISkFAABQCMaPHy9Jqlmzpnr37i03NzcHRwQAAFC8saYUAABAIYqIiNDFixf14YcfKjo6WmfOnJEkbd26VceOHXNwdAAAAMUHM6UAAAAK0Y4dOxQSEiJvb28dOnRIgwcPVoUKFbRo0SIdOXJEc+fOdXSIAAAAxQIzpQAAAArRM888owEDBmj//v02t/B16dJF69evd2BkAAAAxQszpQAAAArRli1b9P777+cpr1atmpKTkx0QEQAAQPHETCkAAIBC5OrqqvT09Dzlv/zyiypXruyAiAAAAIonklIAAACFqHv37po4caIuXbokSbJYLDpy5Iief/559ezZ08HRAQAAFB8kpQAAAArRm2++qXPnzsnHx0cXLlxQ27ZtVadOHZUtW1aTJk1ydHgAAADFBmtKAQAAFCJvb2+tXLlSGzZs0I4dO3Tu3DndddddCgkJcXRoAAAAxQpJKQAAgCLQunVrtW7d2tFhAAAAFFskpQAAAApJTk6O5syZo0WLFunQoUOyWCyqVauWevXqpUceeUQWi8XRIQIAABQbrCkFAABQCAzDUPfu3TVo0CAdO3ZMjRs3VsOGDXX48GENGDBADzzwgKNDBAAAKFaYKQUAAFAI5syZo/Xr1yshIUHt27e32bd69WqFhYVp7ty5Cg8Pd1CEAAAAxQszpQAAAArBZ599ptGjR+dJSElShw4dNGrUKH366acOiAwAAKB4IikFAABQCHbs2KFOnTrlu79z587avn27iREBAAAUbySlAAAACsGZM2fk6+ub735fX1/98ccfJkYEAABQvJGUAgAAKATZ2dkqVSr/5TqdnZ11+fJlEyMCAAAo3orFQudTp07V66+/ruTkZDVt2lTvvvuuAgMD7dZt166d1q1bl6e8S5cuWrp0qSTp3LlzGjVqlBYvXqzTp0+rVq1aGjFihB5//PEibQcAACi5DMPQgAED5Orqand/ZmamyREBAAAUbw5PSsXFxSkqKkozZsxQUFCQYmNjFRoaqn379snHxydP/UWLFikrK8v6+vTp02ratKkefPBBa1lUVJRWr16tTz75RDVr1tSKFSv0xBNPqGrVqurevbsp7QIAACVLRETENevw5D0AAID/sRiGYTgygKCgILVo0UJTpkyRJOXk5Mjf319PPvmkRo0adc3jY2NjNW7cOJ04cUKenp6SpEaNGql3794aO3astV5AQIA6d+6s//73v9c8Z3p6ury9vZWWliYvL68bbBkAALiZcP0vGvQrAAAlT0Gv/w5dUyorK0tJSUkKCQmxljk5OSkkJESJiYkFOsfMmTPVp08fa0JKklq1aqUvv/xSx44dk2EYWrNmjX755Rfde++9ds+RmZmp9PR0mw0AAAAAAABFx6FJqVOnTik7OzvPk2p8fX2VnJx8zeM3bdqkXbt2adCgQTbl7777rho0aKDbbrtNLi4u6tSpk6ZOnao2bdrYPU9MTIy8vb2tm7+//403CgAAAAAAANd0Uz99b+bMmWrcuHGeRdHfffdd/fDDD/ryyy+VlJSkN998U8OGDdOqVavsnic6OlppaWnW7ffffzcjfAAAAAAAgBLLoQudV6pUSc7OzkpJSbEpT0lJkZ+f31WPzcjI0Pz58zVx4kSb8gsXLmj06NH64osv1LVrV0lSkyZNtG3bNr3xxhs2twrmcnV1zfdJOQAAAAAAACh8Dp0p5eLiooCAACUkJFjLcnJylJCQoODg4KseGx8fr8zMTD388MM25ZcuXdKlS5fk5GTbNGdnZ+Xk5BRe8AAAAAAAALhhDp0pJUlRUVGKiIhQ8+bNFRgYqNjYWGVkZCgyMlLSlUcnV6tWTTExMTbHzZw5U2FhYapYsaJNuZeXl9q2bavnnntO7u7uqlGjhtatW6e5c+dq8uTJprULAAAAAAAA+XN4Uqp37946efKkxo0bp+TkZDVr1kzLli2zLn5+5MiRPLOe9u3bpw0bNmjFihV2zzl//nxFR0erf//+OnPmjGrUqKFJkybp8ccfL/L2AAAAAAAA4NoshmEYjg6iuElPT5e3t7fS0tLk5eXl6HAAAIAJuP4XDfoVAICSp6DX/5v66XsAAAAAAAC4OZGUAgAAAAAAgOlISgEAAAAAAMB0JKUAAAAAAABgOpJSAAAAAAAAMB1JKQAAAAAAAJiOpBQAAAAAAABMR1IKAAAAAAAApiMpBQAAAAAAANORlAIAALgFnDlzRv3795eXl5fKlSungQMH6ty5c1c95uLFixo2bJgqVqyoMmXKqGfPnkpJSbHu3759u/r27St/f3+5u7urfv36evvtt4u6KQAAoIQgKQUAAHAL6N+/v37++WetXLlSX3/9tdavX68hQ4Zc9ZhnnnlGX331leLj47Vu3TodP35cPXr0sO5PSkqSj4+PPvnkE/3888964YUXFB0drSlTphR1cwAAQAlgMQzDcHQQxU16erq8vb2VlpYmLy8vR4cDAABMcDNf//fs2aMGDRpo8+bNat68uSRp2bJl6tKli44ePaqqVavmOSYtLU2VK1fWvHnz1KtXL0nS3r17Vb9+fSUmJqply5Z232vYsGHas2ePVq9eXaDYbuZ+BQAAN6ag139mSgEAANzkEhMTVa5cOWtCSpJCQkLk5OSkH3/80e4xSUlJunTpkkJCQqxl9erVU/Xq1ZWYmJjve6WlpalChQr57s/MzFR6errNBgAAYA9JKQAAgJtccnKyfHx8bMpKlSqlChUqKDk5Od9jXFxcVK5cOZtyX1/ffI/5/vvvFRcXd9XbAmNiYuTt7W3d/P39r68xAACgxCApBQAAUEyNGjVKFovlqtvevXtNiWXXrl26//77NX78eN1777351ouOjlZaWpp1+/33302JDwAA3HxKOToAAAAA2Ddy5EgNGDDgqnVuv/12+fn5KTU11ab88uXLOnPmjPz8/Owe5+fnp6ysLJ09e9ZmtlRKSkqeY3bv3q2OHTtqyJAhGjNmzFXjcXV1laur61XrAAAASCSlAAAAiq3KlSurcuXK16wXHByss2fPKikpSQEBAZKk1atXKycnR0FBQXaPCQgIUOnSpZWQkKCePXtKkvbt26cjR44oODjYWu/nn39Whw4dFBERoUmTJhVCqwAAAK7g9j0AAICbXP369dWpUycNHjxYmzZt0saNGzV8+HD16dPH+uS9Y8eOqV69etq0aZMkydvbWwMHDlRUVJTWrFmjpKQkRUZGKjg42PrkvV27dql9+/a69957FRUVpeTkZCUnJ+vkyZMOaysAALh1MFMKAADgFvDpp59q+PDh6tixo5ycnNSzZ0+988471v2XLl3Svn37dP78eWvZW2+9Za2bmZmp0NBQTZs2zbp/4cKFOnnypD755BN98skn1vIaNWro0KFDprQLAADcuiyGYRiODqK4SU9Pl7e3t9LS0uTl5eXocAAAgAm4/hcN+hUAgJKnoNd/bt8DAAAAAACA6UhKAQAAAAAAwHQkpQAAAAAAAGA6klIAAAAAAAAwHUkpAAAAAAAAmI6kFAAAAAAAAExHUgoAAAAAAACmIykFAAAAAAAA05GUAgAAAAAAgOlISgEAAAAAAMB0JKUAAAAAAABgOpJSAAAAAAAAMB1JKQAAAAAAAJiOpBQAAAAAAABMR1IKAAAAAAAApisWSampU6eqZs2acnNzU1BQkDZt2pRv3Xbt2sliseTZunbtaq1jb7/FYtHrr79uRnMAAAAAAABwDQ5PSsXFxSkqKkrjx4/X1q1b1bRpU4WGhio1NdVu/UWLFunEiRPWbdeuXXJ2dtaDDz5orfPX/SdOnNCsWbNksVjUs2dPs5oFAAAAAACAq3B4Umry5MkaPHiwIiMj1aBBA82YMUMeHh6aNWuW3foVKlSQn5+fdVu5cqU8PDxsklJ/3e/n56clS5aoffv2uv32281qFgAAAAAAAK7CoUmprKwsJSUlKSQkxFrm5OSkkJAQJSYmFugcM2fOVJ8+feTp6Wl3f0pKipYuXaqBAwcWSswAAAAAAAD450o58s1PnTql7Oxs+fr62pT7+vpq79691zx+06ZN2rVrl2bOnJlvnY8++khly5ZVjx498q2TmZmpzMxM6+v09PQCRA8AAAAAAIAb5fDb9/6JmTNnqnHjxgoMDMy3zqxZs9S/f3+5ubnlWycmJkbe3t7Wzd/fvyjCBQAAAAAAwP9zaFKqUqVKcnZ2VkpKik15SkqK/Pz8rnpsRkaG5s+ff9Xb8r777jvt27dPgwYNuuq5oqOjlZaWZt1+//33gjcCAAAAAAAA182hSSkXFxcFBAQoISHBWpaTk6OEhAQFBwdf9dj4+HhlZmbq4YcfzrfOzJkzFRAQoKZNm171XK6urvLy8rLZAAAAAAAAUHQcfvteVFSUPvjgA3300Ufas2ePhg4dqoyMDEVGRkqSwsPDFR0dnee4mTNnKiwsTBUrVrR73vT0dMXHx19zlhQAAAAAAADM59CFziWpd+/eOnnypMaNG6fk5GQ1a9ZMy5Ytsy5+fuTIETk52ebO9u3bpw0bNmjFihX5nnf+/PkyDEN9+/Yt0vgBAAAAAABw/SyGYRiODqK4SU9Pl7e3t9LS0riVDwCAEoLrf9GgXwEAKHkKev13+O17AAAAAAAAKHlISgEAAAAAAMB0JKUAAAAAAABgOpJSAAAAAAAAMB1JKQAAAAAAAJiOpBQAAAAAAABMR1IKAAAAAAAApiMpBQAAAAAAANORlAIAAAAAAIDpSEoBAAAAAADAdCSlAAAAAAAAYDqSUgAAAAAAADAdSSkAAAAAAACYjqQUAAAAAAAATEdSCgAAAAAAAKYjKQUAAAAAAADTkZQCAAAAAACA6UhKAQAAAAAAwHQkpQAAAAAAAGA6klIAAAAAAAAwHUkpAAAAAAAAmI6kFAAAAAAAAExHUgoAAAAAAACmIykFAAAAAAAA05GUAgAAAAAAgOlISgEAAAAAAMB0JKUAAAAAAABgOpJSAAAAAAAAMB1JKQAAAAAAAJiOpBQAAAAAAABMR1IKAAAAAAAApiMpBQAAAAAAANORlAIAAAAAAIDpSEoBAAAAAADAdCSlAAAAAAAAYDqSUgAAAAAAADBdsUhKTZ06VTVr1pSbm5uCgoK0adOmfOu2a9dOFoslz9a1a1ebenv27FH37t3l7e0tT09PtWjRQkeOHCnqpgAAADjEmTNn1L9/f3l5ealcuXIaOHCgzp07d9VjLl68qGHDhqlixYoqU6aMevbsqZSUFLt1T58+rdtuu00Wi0Vnz54tghYAAICSxuFJqbi4OEVFRWn8+PHaunWrmjZtqtDQUKWmptqtv2jRIp04ccK67dq1S87OznrwwQetdX799Ve1bt1a9erV09q1a7Vjxw6NHTtWbm5uZjULAADAVP3799fPP/+slStX6uuvv9b69es1ZMiQqx7zzDPP6KuvvlJ8fLzWrVun48ePq0ePHnbrDhw4UE2aNCmK0AEAQAllMQzDcGQAQUFBatGihaZMmSJJysnJkb+/v5588kmNGjXqmsfHxsZq3LhxOnHihDw9PSVJffr0UenSpfXxxx/fUEzp6eny9vZWWlqavLy8bugcAADg5nIzX//37NmjBg0aaPPmzWrevLkkadmyZerSpYuOHj2qqlWr5jkmLS1NlStX1rx589SrVy9J0t69e1W/fn0lJiaqZcuW1rrTp09XXFycxo0bp44dO+qPP/5QuXLlChTbzdyvAADgxhT0+u/QmVJZWVlKSkpSSEiItczJyUkhISFKTEws0DlmzpypPn36WBNSOTk5Wrp0qf71r38pNDRUPj4+CgoK0uLFi/M9R2ZmptLT0202AACAm0ViYqLKlStnTUhJUkhIiJycnPTjjz/aPSYpKUmXLl2yGYfVq1dP1atXtxmH7d69WxMnTtTcuXPl5OTwSfYAAOAW4tCRxalTp5SdnS1fX1+bcl9fXyUnJ1/z+E2bNmnXrl0aNGiQtSw1NVXnzp3TK6+8ok6dOmnFihV64IEH1KNHD61bt87ueWJiYuTt7W3d/P39/1nDAAAATJScnCwfHx+bslKlSqlChQr5jqmSk5Pl4uKSZ8bTX8dhmZmZ6tu3r15//XVVr169QLHwYx8AACiom/rnrpkzZ6px48YKDAy0luXk5EiS7r//fj3zzDNq1qyZRo0apfvuu08zZsywe57o6GilpaVZt99//92U+AEAAK5m1KhRdh/w8tdt7969Rfb+0dHRql+/vh5++OECH8OPfQAAoKBKOfLNK1WqJGdn5zxPeUlJSZGfn99Vj83IyND8+fM1ceLEPOcsVaqUGjRoYFNev359bdiwwe65XF1d5erqegMtAAAAKDojR47UgAEDrlrn9ttvl5+fX56HxFy+fFlnzpzJd0zl5+enrKwsnT171ma21F/HYatXr9bOnTu1cOFCSVLuUqSVKlXSCy+8oAkTJuQ5b3R0tKKioqyv09PTSUwBAAC7HJqUcnFxUUBAgBISEhQWFibpykynhIQEDR8+/KrHxsfHKzMzM88vdy4uLmrRooX27dtnU/7LL7+oRo0ahRo/AABAUapcubIqV658zXrBwcE6e/askpKSFBAQIOlKQiknJ0dBQUF2jwkICFDp0qWVkJCgnj17SpL27dunI0eOKDg4WJL0+eef68KFC9ZjNm/erEcffVTfffedateubfe8/NgHAAAKyqFJKUmKiopSRESEmjdvrsDAQMXGxiojI0ORkZGSpPDwcFWrVk0xMTE2x82cOVNhYWGqWLFinnM+99xz6t27t9q0aaP27dtr2bJl+uqrr7R27VozmgQAAGCq+vXrq1OnTho8eLBmzJihS5cuafjw4erTp4/1yXvHjh1Tx44dNXfuXAUGBsrb21sDBw5UVFSUKlSoIC8vLz355JMKDg62Pnnv74mnU6dOWd+voE/fAwAAyI/Dk1K9e/fWyZMnNW7cOCUnJ6tZs2ZatmyZdfHzI0eO5HnSy759+7RhwwatWLHC7jkfeOABzZgxQzExMRoxYoTq1q2rzz//XK1bty7y9gAAADjCp59+quHDh6tjx45ycnJSz5499c4771j3X7p0Sfv27dP58+etZW+99Za1bmZmpkJDQzVt2jRHhA8AAEogi5G7OACs0tPT5e3trbS0NHl5eTk6HAAAYAKu/0WDfgUAoOQp6PX/pn76HgAAAAAAAG5OJKUAAAAAAABgOpJSAAAAAAAAMB1JKQAAAAAAAJiOpBQAAAAAAABMR1IKAAAAAAAApiMpBQAAAAAAANORlAIAAAAAAIDpSEoBAAAAAADAdCSlAAAAAAAAYDqSUgAAAAAAADAdSSkAAAAAAACYjqQUAAAAAAAATEdSCgAAAAAAAKYjKQUAAAAAAADTkZQCAAAAAACA6UhKAQAAAAAAwHQkpQAAAAAAAGA6klIAAAAAAAAwHUkpAAAAAAAAmI6kFAAAAAAAAExHUgoAAAAAAACmIykFAAAAAAAA05GUAgAAAAAAgOlISgEAAAAAAMB0JKUAAAAAAABgOpJSAAAAAAAAMB1JKQAAAAAAAJiOpBQAAAAAAABMR1IKAAAAAAAApiMpBQAAAAAAANORlAIAAAAAAIDpSEoBAAAAAADAdCSlAAAAAAAAYDqSUgAAAAAAADBdsUhKTZ06VTVr1pSbm5uCgoK0adOmfOu2a9dOFoslz9a1a1drnQEDBuTZ36lTJzOaAgAAAAAAgAIo5egA4uLiFBUVpRkzZigoKEixsbEKDQ3Vvn375OPjk6f+okWLlJWVZX19+vRpNW3aVA8++KBNvU6dOmn27NnW166urkXXCAAAAAAAAFwXh8+Umjx5sgYPHqzIyEg1aNBAM2bMkIeHh2bNmmW3foUKFeTn52fdVq5cKQ8PjzxJKVdXV5t65cuXN6M5AAAAAAAAKACHJqWysrKUlJSkkJAQa5mTk5NCQkKUmJhYoHPMnDlTffr0kaenp0352rVr5ePjo7p162ro0KE6ffp0vufIzMxUenq6zQYAAAAAAICi49Ck1KlTp5SdnS1fX1+bcl9fXyUnJ1/z+E2bNmnXrl0aNGiQTXmnTp00d+5cJSQk6NVXX9W6devUuXNnZWdn2z1PTEyMvL29rZu/v/+NNwoAAAAAAADX5PA1pf6JmTNnqnHjxgoMDLQp79Onj/X/GzdurCZNmqh27dpau3atOnbsmOc80dHRioqKsr5OT08nMQUAAAAAAFCEHDpTqlKlSnJ2dlZKSopNeUpKivz8/K56bEZGhubPn6+BAwde831uv/12VapUSQcOHLC739XVVV5eXjYbAAAAAAAAio5Dk1IuLi4KCAhQQkKCtSwnJ0cJCQkKDg6+6rHx8fHKzMzUww8/fM33OXr0qE6fPq0qVar845gBAAAAAADwzzn86XtRUVH64IMP9NFHH2nPnj0aOnSoMjIyFBkZKUkKDw9XdHR0nuNmzpypsLAwVaxY0ab83Llzeu655/TDDz/o0KFDSkhI0P333686deooNDTUlDYBAAAAAADg6hy+plTv3r118uRJjRs3TsnJyWrWrJmWLVtmXfz8yJEjcnKyzZ3t27dPGzZs0IoVK/Kcz9nZWTt27NBHH32ks2fPqmrVqrr33nv10ksvydXV1ZQ2AQAAAAAA4OoshmEYjg6iuElPT5e3t7fS0tJYXwoAgBKC63/RoF8BACh5Cnr9d/jtewAAAAAAACh5SEoBAAAAAADAdA5fU6o4yr2jMT093cGRAAAAs+Re91nZoHAxrgIAoOQp6LiKpJQdf/75pyTJ39/fwZEAAACz/fnnn/L29nZ0GLcMxlUAAJRc1xpXsdC5HTk5OTp+/LjKli0ri8Xi6HCKjfT0dPn7++v3339noVKT0Ofmo8/NR5+bjz63zzAM/fnnn6patWqeJ//ixjGuso8/h+ajz81Hn5uPPjcffW5fQcdVzJSyw8nJSbfddpujwyi2vLy8+MNmMvrcfPS5+ehz89HneTFDqvAxrro6/hyajz43H31uPvrcfPR5XgUZV/EzIAAAAAAAAExHUgoAAAAAAACmIymFAnN1ddX48ePl6urq6FBKDPrcfPS5+ehz89HngOPx59B89Ln56HPz0efmo8//GRY6BwAAAAAAgOmYKQUAAAAAAADTkZQCAAAAAACA6UhKAQAAAAAAwHQkpWB15swZ9e/fX15eXipXrpwGDhyoc+fOXfWYixcvatiwYapYsaLKlCmjnj17KiUlxW7d06dP67bbbpPFYtHZs2eLoAU3n6Lo8+3bt6tv377y9/eXu7u76tevr7fffruom1KsTZ06VTVr1pSbm5uCgoK0adOmq9aPj49XvXr15ObmpsaNG+ubb76x2W8YhsaNG6cqVarI3d1dISEh2r9/f1E24aZTmH1+6dIlPf/882rcuLE8PT1VtWpVhYeH6/jx40XdjJtKYX/P/+rxxx+XxWJRbGxsIUcN3LoYV5mPcZU5GFeZj3GV+RhXmcgA/l+nTp2Mpk2bGj/88IPx3XffGXXq1DH69u171WMef/xxw9/f30hISDC2bNlitGzZ0mjVqpXduvfff7/RuXNnQ5Lxxx9/FEELbj5F0eczZ840RowYYaxdu9b49ddfjY8//thwd3c33n333aJuTrE0f/58w8XFxZg1a5bx888/G4MHDzbKlStnpKSk2K2/ceNGw9nZ2XjttdeM3bt3G2PGjDFKly5t7Ny501rnlVdeMby9vY3Fixcb27dvN7p3727UqlXLuHDhglnNKtYKu8/Pnj1rhISEGHFxccbevXuNxMREIzAw0AgICDCzWcVaUXzPcy1atMho2rSpUbVqVeOtt94q4pYAtw7GVeZjXFX0GFeZj3GV+RhXmYukFAzDMIzdu3cbkozNmzdby7799lvDYrEYx44ds3vM2bNnjdKlSxvx8fHWsj179hiSjMTERJu606ZNM9q2bWskJCQwePp/Rd3nf/XEE08Y7du3L7zgbyKBgYHGsGHDrK+zs7ONqlWrGjExMXbrP/TQQ0bXrl1tyoKCgozHHnvMMAzDyMnJMfz8/IzXX3/duv/s2bOGq6ur8dlnnxVBC24+hd3n9mzatMmQZBw+fLhwgr7JFVWfHz161KhWrZqxa9cuo0aNGgyegAJiXGU+xlXmYFxlPsZV5mNcZS5u34MkKTExUeXKlVPz5s2tZSEhIXJyctKPP/5o95ikpCRdunRJISEh1rJ69eqpevXqSkxMtJbt3r1bEydO1Ny5c+XkxFcuV1H2+d+lpaWpQoUKhRf8TSIrK0tJSUk2/eXk5KSQkJB8+ysxMdGmviSFhoZa6x88eFDJyck2dby9vRUUFHTVz6CkKIo+tyctLU0Wi0XlypUrlLhvZkXV5zk5OXrkkUf03HPPqWHDhkUTPHCLYlxlPsZVRY9xlfkYV5mPcZX5uJJBkpScnCwfHx+bslKlSqlChQpKTk7O9xgXF5c8f3n5+vpaj8nMzFTfvn31+uuvq3r16kUS+82qqPr8777//nvFxcVpyJAhhRL3zeTUqVPKzs6Wr6+vTfnV+is5Ofmq9XP/ez3nLEmKos//7uLFi3r++efVt29feXl5FU7gN7Gi6vNXX31VpUqV0ogRIwo/aOAWx7jKfIyrih7jKvMxrjIf4yrzkZS6xY0aNUoWi+Wq2969e4vs/aOjo1W/fn09/PDDRfYexY2j+/yvdu3apfvvv1/jx4/Xvffea8p7AkXp0qVLeuihh2QYhqZPn+7ocG5ZSUlJevvttzVnzhxZLBZHhwMUG46+xjOuYlwFFCbGVeZgXHV1pRwdAIrWyJEjNWDAgKvWuf322+Xn56fU1FSb8suXL+vMmTPy8/Oze5yfn5+ysrJ09uxZm1+YUlJSrMesXr1aO3fu1MKFCyVdebqGJFWqVEkvvPCCJkyYcIMtK74c3ee5du/erY4dO2rIkCEaM2bMDbXlZlepUiU5OzvneXKRvf7K5efnd9X6uf9NSUlRlSpVbOo0a9asEKO/ORVFn+fKHTgdPnxYq1ev5te8/1cUff7dd98pNTXVZiZGdna2Ro4cqdjYWB06dKhwGwHcJBx9jWdcZR/jKnMwrjIf4yrzMa5yAMcuaYXiIndxyC1btljLli9fXqDFIRcuXGgt27t3r83ikAcOHDB27txp3WbNmmVIMr7//vt8n15QUhRVnxuGYezatcvw8fExnnvuuaJrwE0iMDDQGD58uPV1dna2Ua1atasuVHjffffZlAUHB+dZkPONN96w7k9LS2NBzr8o7D43DMPIysoywsLCjIYNGxqpqalFE/hNrLD7/NSpUzZ/d+/cudOoWrWq8fzzzxt79+4tuoYAtwjGVeZjXGUOxlXmY1xlPsZV5iIpBatOnToZd955p/Hjjz8aGzZsMO644w6bx+gePXrUqFu3rvHjjz9ayx5//HGjevXqxurVq40tW7YYwcHBRnBwcL7vsWbNGp4S8xdF0ec7d+40KleubDz88MPGiRMnrFtJveDMnz/fcHV1NebMmWPs3r3bGDJkiFGuXDkjOTnZMAzDeOSRR4xRo0ZZ62/cuNEoVaqU8cYbbxh79uwxxo8fb/fRxeXKlTOWLFli7Nixw7j//vt5dPFfFHafZ2VlGd27dzduu+02Y9u2bTbf68zMTIe0sbgpiu/53/GUGOD6MK4yH+Oqose4ynyMq8zHuMpcJKVgdfr0aaNv375GmTJlDC8vLyMyMtL4888/rfsPHjxoSDLWrFljLbtw4YLxxBNPGOXLlzc8PDyMBx54wDhx4kS+78HgyVZR9Pn48eMNSXm2GjVqmNiy4uXdd981qlevbri4uBiBgYHGDz/8YN3Xtm1bIyIiwqb+ggULjH/961+Gi4uL0bBhQ2Pp0qU2+3NycoyxY8cavr6+hqurq9GxY0dj3759ZjTlplGYfZ7758De9tc/GyVdYX/P/47BE3B9GFeZj3GVORhXmY9xlfkYV5nHYhj/fzM6AAAAAAAAYBKevgcAAAAAAADTkZQCAAAAAACA6UhKAQAAAAAAwHQkpQAAAAAAAGA6klIAAAAAAAAwHUkpAAAAAAAAmI6kFAAAAAAAAExHUgoAAAAAAACmIykFAIXAYrFo8eLFjg4DAADgpse4Cig5SEoBuOkNGDBAFoslz9apUydHhwYAAHBTYVwFwEylHB0AABSGTp06afbs2TZlrq6uDooGAADg5sW4CoBZmCkF4Jbg6uoqPz8/m618+fKSrkwBnz59ujp37ix3d3fdfvvtWrhwoc3xO3fuVIcOHeTu7q6KFStqyJAhOnfunE2dWbNmqWHDhnJ1dVWVKlU0fPhwm/2nTp3SAw88IA8PD91xxx368ssvi7bRAAAARYBxFQCzkJQCUCKMHTtWPXv21Pbt29W/f3/16dNHe/bskSRlZGQoNDRU5cuX1+bNmxUfH69Vq1bZDI6mT5+uYcOGaciQIdq5c6e+/PJL1alTx+Y9JkyYoIceekg7duxQly5d1L9/f505c8bUdgIAABQ1xlUACo0BADe5iIgIw9nZ2fD09LTZJk2aZBiGYUgyHn/8cZtjgoKCjKFDhxqGYRjvv/++Ub58eePcuXPW/UuXLjWcnJyM5ORkwzAMo2rVqsYLL7yQbwySjDFjxlhfnzt3zpBkfPvtt4XWTgAAgKLGuAqAmVhTCsAtoX379po+fbpNWYUKFaz/HxwcbLMvODhY27ZtkyTt2bNHTZs2laenp3X/3XffrZycHO3bt08Wi0XHjx9Xx44drxpDkyZNrP/v6ekpLy8vpaam3miTAAAAHIJxFQCzkJQCcEvw9PTMM+27sLi7uxeoXunSpW1eWywW5eTkFEVIAAAARYZxFQCzsKYUgBLhhx9+yPO6fv36kqT69etr+/btysjIsO7fuHGjnJycVLduXZUtW1Y1a9ZUQkKCqTEDAAAUR4yrABQWZkoBuCVkZmYqOTnZpqxUqVKqVKmSJCk+Pl7NmzdX69at9emnn2rTpk2aOXOmJKl///4aP368IiIi9OKLL+rkyZN68skn9cgjj8jX11eS9OKLL+rxxx+Xj4+POnfurD///FMbN27Uk08+aW5DAQAAihjjKgBmISkF4JawbNkyValSxaasbt262rt3r6QrT3CZP3++nnjiCVWpUkWfffaZGjRoIEny8PDQ8uXL9dRTT6lFixby8PBQz549NXnyZOu5IiIidPHiRb311lt69tlnValSJfXq1cu8BgIAAJiEcRUAs1gMwzAcHQQAFCWLxaIvvvhCYWFhjg4FAADgpsa4CkBhYk0pAAAAAAAAmI6kFAAAAAAAAEzH7XsAAAAAAAAwHTOlAAAAAAAAYDqSUgAAAAAAADAdSSkAAAAAAACYjqQUAAAAAAAATEdSCgAAAAAAAKYjKQUAAAAAAADTkZQCAAAAAACA6UhKAQAAAAAAwHQkpQAAAAAAAGC6/wMqEOQ5fxOGEwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['val_detected_rate'], label='Validation Detection Rate')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Detection Rate')\n",
    "plt.title('Validation Detection Rate Over Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce122dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), 'fasterrcnn_resnet50_inflamm_cell.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72635ba6",
   "metadata": {},
   "source": [
    "### Prediction visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98418035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def visualize_patch_grid(results, patches_dir, class_folders, patch_list, cols=3, figsize=(12,8), score_thresh=0.5):\n",
    "    \"\"\"\n",
    "    results:      dict mapping filename -> list of (x1,y1,x2,y2,score)\n",
    "    patches_dir:  Path to parent of class subfolders (e.g. .../cpg)\n",
    "    class_folders: list of your subfolder names, e.g. [\"inflammatory-cells\", \"monocytes\", \"lymphocytes\"]\n",
    "    patch_list:   list of filenames you want to plot, e.g. [\"P000026_monocytes_patch0.png\", ...]\n",
    "    cols:         how many columns in the grid\n",
    "    \"\"\"\n",
    "    rows = math.ceil(len(patch_list)/cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax, fname in zip(axes, patch_list):\n",
    "        # locate the file under one of the class folders\n",
    "        img_path = None\n",
    "        for cls in class_folders:\n",
    "            p = patches_dir/cls/fname\n",
    "            if p.exists():\n",
    "                img_path = p\n",
    "                break\n",
    "        if img_path is None:\n",
    "            raise FileNotFoundError(f\"Couldn’t find {fname} in any subfolder of {patches_dir}\")\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(fname, fontsize=9)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        # draw each box above the threshold\n",
    "        for (x1,y1,x2,y2,sc) in results.get(fname, []):\n",
    "            if sc < score_thresh:\n",
    "                continue\n",
    "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                                     linewidth=2, edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x1, y1-3, f\"{sc:.2f}\",\n",
    "                    color='yellow', fontsize=7,\n",
    "                    bbox=dict(facecolor='black', alpha=0.6, pad=1))\n",
    "\n",
    "    # turn off any unused axes\n",
    "    for ax in axes[len(patch_list):]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f05a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick 6 random patches you want to inspect\n",
    "from pathlib import Path\n",
    "\n",
    "PATCHES_DIR   = Path(r\"C:\\Users\\lisan\\AIMI_PROJECT\\patches\\cpg\")\n",
    "CLASS_FOLDERS = [\"inflammatory-cells\", \"monocytes\", \"lymphocytes\"]\n",
    "\n",
    "to_plot = [\n",
    "    \"P000026_monocytes_patch0.png\",\n",
    "    \"P000026_monocytes_patch3.png\",\n",
    "    \"P000006_inflammatory-cells_patch0.png\",\n",
    "    \"P000010_lymphocytes_patch3.png\",\n",
    "    \"P000002_inflammatory-cells_patch0.png\",\n",
    "    \"P000004_monocytes_patch1.png\",\n",
    "]\n",
    "\n",
    "visualize_patch_grid(\n",
    "#    results,  \n",
    "    loaded_results,\n",
    "    PATCHES_DIR,\n",
    "    CLASS_FOLDERS,\n",
    "    patch_list=to_plot,\n",
    "    cols=3,\n",
    "    figsize=(12, 8),\n",
    "    score_thresh=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52398a5",
   "metadata": {},
   "source": [
    "### Comparison to GT visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fd9328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# load your json_pixel ground truth\n",
    "PIXEL_JSON_DIR = Path(r\"C:\\Users\\lisan\\monkey-data\\annotations\\json_pixel\\json_pixel\")\n",
    "pixel_coords = {}   # (slide,cls) -> list of (x, y)\n",
    "for jf in PIXEL_JSON_DIR.glob(\"A_*_*.json\"):\n",
    "    parts = jf.stem.split(\"_\")          # [\"A\",\"P000001\",\"inflammatory-cells\"]\n",
    "    slide = parts[1]\n",
    "    cls   = \"_\".join(parts[2:])\n",
    "    data  = json.loads(jf.read_text())\n",
    "    pts   = [pt[\"point\"] for pt in data[\"points\"]]\n",
    "    pixel_coords[(slide, cls)] = pts\n",
    "\n",
    "# recover patch origin in full‐slide coords\n",
    "def get_patch_origin(patch_name: str):\n",
    "    \"\"\"\n",
    "    Given \"P000001_inflammatory-cells_patch42.png\",\n",
    "    looks up point #42 in pixel_coords and assumes that\n",
    "    patch size was 256×256 centered on that point.\n",
    "    \"\"\"\n",
    "    slide, cls, rest = patch_name.split(\"_\", 2)\n",
    "    idx = int(rest.replace(\"patch\", \"\").replace(\".png\", \"\"))\n",
    "    xg, yg = pixel_coords[(slide, cls)][idx]\n",
    "    # patch was 256×256 centered → top‐left is (xg–128, yg–128)\n",
    "    return int(xg - PATCH_SIZE//2), int(yg - PATCH_SIZE//2)\n",
    "\n",
    "# IoU helper\n",
    "def iou(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interW = max(0, xB - xA)\n",
    "    interH = max(0, yB - yA)\n",
    "    inter = interW * interH\n",
    "    areaA = (boxA[2]-boxA[0])*(boxA[3]-boxA[1])\n",
    "    areaB = (boxB[2]-boxB[0])*(boxB[3]-boxB[1])\n",
    "    union = areaA + areaB - inter\n",
    "    return inter/union if union>0 else 0\n",
    "\n",
    "# evaluate\n",
    "TP = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "score_thr = SCORE_THRESH   # 0.5\n",
    "iou_thr   = 0.5\n",
    "\n",
    "# for every GT patch\n",
    "for (slide, cls), pts in pixel_coords.items():\n",
    "    for idx, (xg, yg) in enumerate(pts):\n",
    "        patch_name = f\"{slide}_{cls}_patch{idx}.png\"\n",
    "        x0, y0 = get_patch_origin(patch_name)\n",
    "        # build the single GT box in local patch coords:\n",
    "        lx, ly = xg - x0, yg - y0\n",
    "        gt_box = [lx, ly, lx + BBOX_SIZE, ly + BBOX_SIZE]\n",
    "\n",
    "        preds = results.get(patch_name, [])\n",
    "        # if no preds above score_thr at all → false negative\n",
    "        if not any(p[4] >= score_thr for p in preds):\n",
    "            FN += 1\n",
    "            continue\n",
    "\n",
    "        # otherwise compare each pred to the GT\n",
    "        matched = False\n",
    "        for x1,y1,x2,y2,sc in preds:\n",
    "            if sc < score_thr:\n",
    "                continue\n",
    "            pred_box = [x1,y1,x2,y2]\n",
    "            if iou(pred_box, gt_box) >= iou_thr:\n",
    "                TP += 1\n",
    "                matched = True\n",
    "            else:\n",
    "                FP += 1\n",
    "        if not matched:\n",
    "            FN += 1\n",
    "\n",
    "# any preds on patches that have no GT file should count as FP\n",
    "for patch_name, preds in results.items():\n",
    "    slide, cls, _ = patch_name.split(\"_\", 2)\n",
    "    if (slide, cls) not in pixel_coords:\n",
    "        for _,_,_,_,sc in preds:\n",
    "            if sc >= score_thr:\n",
    "                FP += 1\n",
    "\n",
    "precision = TP / (TP + FP) if (TP + FP) else 0.0\n",
    "recall    = TP / (TP + FN) if (TP + FN) else 0.0\n",
    "print(f\"TP={TP}, FP={FP}, FN={FN}\")\n",
    "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57dda55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "\n",
    "# ─── CONFIG ─────────────────────────────────────────────────\n",
    "PATCHES_DIR   = Path(r\"C:\\Users\\lisan\\AIMI_PROJECT\\patches\\cpg\")\n",
    "JSON_DIR      = Path(r\"C:\\Users\\lisan\\monkey-data\\annotations\\json_pixel\\json_pixel\")\n",
    "PATCH_SIZE    = 256\n",
    "BBOX_SIZE     = 32\n",
    "\n",
    "# load all JSON‐pixel GT points\n",
    "def load_json_pixels(json_dir):\n",
    "    d = {}\n",
    "    for jf in Path(json_dir).glob(\"A_*_*.json\"):\n",
    "        slide, cls = jf.stem.split(\"_\")[1], jf.stem.split(\"_\",2)[2]\n",
    "        pts = json.loads(jf.read_text())[\"points\"]\n",
    "        d[(slide,cls)] = [tuple(p[\"point\"]) for p in pts]\n",
    "    return d\n",
    "\n",
    "def infer_n_cols(slide: str, cls: str) -> int:\n",
    "    \"\"\"\n",
    "    Look at how many patches of this (slide,cls) exist before the row rolls over.\n",
    "    \"\"\"\n",
    "    folder = PATCHES_DIR / cls\n",
    "    idxs = sorted(\n",
    "        int(p.stem.split(\"_\")[-1].replace(\"patch\",\"\"))\n",
    "        for p in folder.glob(f\"{slide}_{cls}_patch*.png\")\n",
    "    )\n",
    "    for expected, actual in enumerate(idxs):\n",
    "        if actual != expected:\n",
    "            return expected\n",
    "    return len(idxs)\n",
    "\n",
    "\n",
    "def get_patch_origin(patch_name: str):\n",
    "    \"\"\"\n",
    "    Given 'P000001_inflammatory-cells_patch42.png', look up\n",
    "    the 43rd GT point in pixel_coords, and back out the\n",
    "    256×256 window that was centered on it.\n",
    "    \"\"\"\n",
    "    slide, cls, rest = patch_name.split(\"_\", 2)\n",
    "    idx = int(rest.replace(\"patch\",\"\").replace(\".png\",\"\"))\n",
    "    # fetch the GT center for this patch\n",
    "    xg, yg = pixel_coords[(slide, cls)][idx]\n",
    "    # roll that point back to the top‐left of a 256×256 window\n",
    "    x0 = xg - PATCH_SIZE//2\n",
    "    y0 = yg - PATCH_SIZE//2\n",
    "    return x0, y0\n",
    "\n",
    "# new visualize function\n",
    "def visualize_overlay(patch_list, score_thresh=0.5):\n",
    "    for fname in patch_list:\n",
    "        slide, cls, _ = fname.split(\"_\", 2)\n",
    "        img_path = PATCHES_DIR / cls / fname\n",
    "        if not img_path.exists():\n",
    "            print(f\"  {img_path} not found, skipping\")\n",
    "            continue\n",
    "\n",
    "        #img = Image.open(img_path).convert(\"RGB\")\n",
    "        #fig, ax = plt.subplots(1,1, figsize=(4,4))\n",
    "        #ax.imshow(img)\n",
    "        #ax.axis(\"off\")\n",
    "        #ax.set_title(fname, fontsize=8)\n",
    "\n",
    "        x0, y0 = get_patch_origin(fname)   \n",
    "\n",
    "        # now plot\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        fig,ax = plt.subplots(1,1,figsize=(4,4))\n",
    "        ax.imshow(img); ax.axis(\"off\"); ax.set_title(fname,fontsize=8)\n",
    "\n",
    "        # draw GT boxes\n",
    "        for xg,yg in pixel_coords.get((slide,cls),[]):\n",
    "            lx,ly = xg-x0, yg-y0\n",
    "            if 0<=lx<PATCH_SIZE and 0<=ly<PATCH_SIZE:\n",
    "                ax.add_patch(patches.Rectangle(\n",
    "                    (lx,ly), BBOX_SIZE,BBOX_SIZE,\n",
    "                    edgecolor=\"g\",facecolor=\"none\",linewidth=2,\n",
    "                                label=\"All GT annotations\"\n",
    "\n",
    "                ))\n",
    "        \n",
    "        # draw just the one central GT box you trained on\n",
    "        cx = PATCH_SIZE//2 - BBOX_SIZE//2\n",
    "        cy = PATCH_SIZE//2 - BBOX_SIZE//2\n",
    "        ax.add_patch(patches.Rectangle(\n",
    "            (cx, cy), BBOX_SIZE, BBOX_SIZE,\n",
    "            edgecolor=\"m\", facecolor=\"none\", linewidth=2,\n",
    "                label=\"Extraction window\"\n",
    "\n",
    "        ))    \n",
    "\n",
    "        # compute origin and patch‐idx\n",
    "        x0, y0 = get_patch_origin(fname)\n",
    "        idx   = int(fname.split(\"_\")[-1].replace(\"patch\",\"\").replace(\".png\",\"\"))\n",
    "        \n",
    "        # BLUE SQUARE: get exactly the pixel you centered on. visualize the one GT point I used to extract that patch\n",
    "        \n",
    "        xg, yg = pixel_coords[(slide,cls)][idx]\n",
    "        lx, ly = xg - x0, yg - y0\n",
    "        \n",
    "        if 0 <= lx < PATCH_SIZE and 0 <= ly < PATCH_SIZE:\n",
    "            ax.add_patch(patches.Rectangle(\n",
    "                (lx, ly), BBOX_SIZE, BBOX_SIZE,\n",
    "                edgecolor=\"b\", facecolor=\"none\", linewidth=2,\n",
    "                        label=\"Reference annotation\"\n",
    "\n",
    "            ))\n",
    "\n",
    "        # draw your saved `results` preds in red\n",
    "        for (x1,y1,x2,y2,sc) in results.get(fname, []):\n",
    "            if sc < score_thresh: continue\n",
    "            ax.add_patch(patches.Rectangle(\n",
    "                (x1, y1),\n",
    "                x2-x1, y2-y1,\n",
    "                edgecolor=\"r\", facecolor=\"none\", linewidth=2,\n",
    "                        label=\"Model prediction\"\n",
    "\n",
    "            ))\n",
    "            ax.text(x1, y1-2, f\"{sc:.2f}\",\n",
    "                    color=\"r\", fontsize=6,\n",
    "                    bbox=dict(facecolor=\"white\", alpha=0.6, pad=1))\n",
    "\n",
    "        # add clean legend\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        by_label = dict(zip(labels, handles))\n",
    "        ax.legend(by_label.values(), by_label.keys(),\n",
    "                  loc=\"upper right\", fontsize=6, framealpha=0.8)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c780dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ─── HOW TO CALL IT ────────────────────────────────────────────\n",
    "# only visualize patches you actually have predictions for:\n",
    "\n",
    "pixel_coords = load_json_pixels(JSON_DIR)\n",
    "\n",
    "patch_list = list(results.keys())[:6]   # e.g. first 6 patches\n",
    "visualize_overlay(patch_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bc943a",
   "metadata": {},
   "source": [
    "### Evaluate detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969db8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def evaluate_detections(results,\n",
    "                        pixel_coords,\n",
    "                        get_patch_origin,\n",
    "                        score_thr=0.5):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "\n",
    "    # for each patch\n",
    "    for patch_name, dets in results.items():\n",
    "        slide, cls, _ = patch_name.split(\"_\",2)\n",
    "        gt_pts = pixel_coords.get((slide,cls), [])\n",
    "        if not gt_pts:\n",
    "            continue\n",
    "\n",
    "        # shift all GTs into patch‐local coords\n",
    "        x0,y0 = get_patch_origin(patch_name)\n",
    "        local_gts = [(x-x0, y-y0) for x,y in gt_pts]\n",
    "\n",
    "        # which GTs have been “covered”?\n",
    "        gt_covered = [False]*len(local_gts)\n",
    "\n",
    "        # for each pred box above threshold\n",
    "        for x1,y1,x2,y2,sc in dets:\n",
    "            if sc < score_thr:\n",
    "                continue\n",
    "\n",
    "            # see if it covers *any* GT\n",
    "            hit = False\n",
    "            for i,(gx,gy) in enumerate(local_gts):\n",
    "                if gx>=x1 and gx<=x2 and gy>=y1 and gy<=y2:\n",
    "                    hit = True\n",
    "                    gt_covered[i] = True\n",
    "            if hit:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "\n",
    "        # any GTs left uncovered?\n",
    "        FN += sum(1 for covered in gt_covered if not covered)\n",
    "\n",
    "    precision = TP / (TP+FP) if TP+FP>0 else 0.0\n",
    "    recall    = TP / (TP+FN) if TP+FN>0 else 0.0\n",
    "    f1        = 2*precision*recall/(precision+recall) if precision+recall>0 else 0.0\n",
    "\n",
    "    return {\"TP\":TP, \"FP\":FP, \"FN\":FN,\n",
    "            \"precision\":precision,\n",
    "            \"recall\":recall,\n",
    "            \"f1\":f1}\n",
    "\n",
    "# --- run it ---\n",
    "metrics = evaluate_detections(\n",
    "    results=results,\n",
    "    pixel_coords=pixel_coords,\n",
    "    get_patch_origin=get_patch_origin,\n",
    "    score_thr=0.5\n",
    ")\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd95fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def visualize_patch_with_all_gt(fname, results, pixel_coords):\n",
    "    slide, cls, _ = fname.split(\"_\", 2)\n",
    "    img_path = PATCHES_DIR / cls / fname\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    # find patch origin and patch‐idx\n",
    "    x0, y0 = get_patch_origin(fname)\n",
    "    idx    = int(fname.split(\"_\")[-1].replace(\"patch\",\"\").replace(\".png\",\"\"))\n",
    "\n",
    "    # pull out all JSON points for this slide/class\n",
    "    all_pts = pixel_coords[(slide,cls)]\n",
    "\n",
    "    fig, ax = plt.subplots(1,1,figsize=(4,4))\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(fname, fontsize=8)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # Big green box at your one “centered” GT\n",
    "    xg, yg = all_pts[idx]\n",
    "    lx, ly = xg - x0, yg - y0\n",
    "    if 0 <= lx < PATCH_SIZE and 0 <= ly < PATCH_SIZE:\n",
    "        ax.add_patch(patches.Rectangle(\n",
    "            (lx, ly), BBOX_SIZE, BBOX_SIZE,\n",
    "            edgecolor=\"g\", facecolor=\"none\", linewidth=2,\n",
    "            label=\"training GT\"\n",
    "        ))\n",
    "\n",
    "    # 2) Blue dots for *all* other JSON points that land in this patch\n",
    "    for (xpi, ypi) in all_pts:\n",
    "        lxi, lyi = xpi - x0, ypi - y0\n",
    "        if 0 <= lxi < PATCH_SIZE and 0 <= lyi < PATCH_SIZE:\n",
    "            ax.plot(lxi, lyi, \"bo\", markersize=3, alpha=0.5)\n",
    "\n",
    "    # 3) Your model’s preds in red\n",
    "    for (x1,y1,x2,y2,sc) in results.get(fname, []):\n",
    "        if sc < 0.5: continue\n",
    "        ax.add_patch(patches.Rectangle(\n",
    "            (x1, y1), x2-x1, y2-y1,\n",
    "            edgecolor=\"r\", facecolor=\"none\", linewidth=1\n",
    "        ))\n",
    "        ax.text(x1, y1-2, f\"{sc:.2f}\",\n",
    "                color=\"r\", fontsize=6,\n",
    "                bbox=dict(facecolor=\"white\", alpha=0.6, pad=1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc=\"upper right\", fontsize=6)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9a5281",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "patch_list = list(results.keys())[:10]\n",
    "\n",
    "for fname in patch_list:\n",
    "    visualize_patch_with_all_gt(fname, results, pixel_coords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
