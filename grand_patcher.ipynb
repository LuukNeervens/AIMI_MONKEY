{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49212915",
   "metadata": {},
   "source": [
    "# Try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17679f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import tifffile as tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27cbdc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Config -----------\n",
    "\n",
    "DATASET_ROOT = r\"C:\\Users\\luukn\\AIMI_MONKEY2\\monkey-training\"\n",
    "\n",
    "train_dirs = [\n",
    "    os.path.join(DATASET_ROOT, \"images/pas-original\"),\n",
    "    os.path.join(DATASET_ROOT, \"images/pas-diagnostic\"),\n",
    "]\n",
    "\n",
    "annotation_dir = os.path.join(DATASET_ROOT, \"annotations/json_pixel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dc442cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Helper Functions -----------\n",
    "\n",
    "def parse_annotations(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    coords = [(int(p[\"point\"][0]), int(p[\"point\"][1])) for p in data[\"points\"]]\n",
    "    return coords\n",
    "\n",
    "\n",
    "def extract_patches(image, stride, patch_size):\n",
    "    h, w, _ = image.shape\n",
    "    patches = []\n",
    "    coords = []\n",
    "    for y in range(0, h - patch_size + 1, stride):\n",
    "        for x in range(0, w - patch_size + 1, stride):\n",
    "            patch = image[y:y + patch_size, x:x + patch_size]\n",
    "            patches.append(patch)\n",
    "            coords.append((x, y))\n",
    "    return patches, coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1702ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Transform -----------\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((256, 256)),  # or your patch size\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1de0eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "\n",
    "class InflammatoryCellDataset(Dataset):\n",
    "    def __init__(self, image_dir_list, annotation_dir, patch_size_px=256, num_patches=256):\n",
    "        self.samples = []\n",
    "        self.patch_size_px = patch_size_px\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        for img_dir in image_dir_list:\n",
    "            print(f\"\\nScanning: {img_dir}\")\n",
    "            image_files = glob(os.path.join(img_dir, \"*.tif\"))\n",
    "            print(f\"Found {len(image_files)} image(s)\")\n",
    "\n",
    "            for img_path in image_files:\n",
    "                print(f\"Processing: {img_path}\")\n",
    "                filename = os.path.basename(img_path)\n",
    "                slide_id = \"_\".join(filename.split('_')[:2])  # e.g. A_P000001\n",
    "                ann_path = os.path.join(annotation_dir, f\"{slide_id}_inflammatory-cells.json\")\n",
    "\n",
    "                if not os.path.exists(ann_path):\n",
    "                    print(f\"❌ Annotation not found: {ann_path}\")\n",
    "                    continue\n",
    "\n",
    "                ann_coords = parse_annotations(ann_path)\n",
    "\n",
    "                with tifffile.TiffFile(img_path) as tif:\n",
    "                    page = tif.pages[0]\n",
    "                    mm = page.asarray(memmap=True)  # memory-mapped numpy array\n",
    "                    h, w = mm.shape[:2]\n",
    "\n",
    "                    patches_sampled = 0\n",
    "                    attempts = 0\n",
    "                    max_attempts = self.num_patches * 10\n",
    "\n",
    "                    while patches_sampled < self.num_patches and attempts < max_attempts:\n",
    "                        x_offset = random.randint(0, w - self.patch_size_px)\n",
    "                        y_offset = random.randint(0, h - self.patch_size_px)\n",
    "\n",
    "                        patch = mm[y_offset:y_offset + self.patch_size_px, x_offset:x_offset + self.patch_size_px]\n",
    "\n",
    "                        # If grayscale, convert to 3 channels\n",
    "                        if patch.ndim == 2:\n",
    "                            patch = np.stack([patch]*3, axis=-1)\n",
    "                        elif patch.shape[2] == 4:\n",
    "                            patch = patch[:, :, :3]\n",
    "\n",
    "                        # Find annotations inside patch\n",
    "                        local_targets = []\n",
    "                        for (gx, gy) in ann_coords:\n",
    "                            if x_offset <= gx < x_offset + self.patch_size_px and y_offset <= gy < y_offset + self.patch_size_px:\n",
    "                                local_x = gx - x_offset\n",
    "                                local_y = gy - y_offset\n",
    "                                local_targets.append((local_x, local_y))\n",
    "\n",
    "                        if local_targets:\n",
    "                            for t in local_targets:\n",
    "                                self.samples.append((patch, t))\n",
    "                            patches_sampled += 1\n",
    "\n",
    "                        attempts += 1\n",
    "\n",
    "                print(f\"Sampled {patches_sampled} patches from {img_path}\")\n",
    "\n",
    "        print(f\"✅ Total training samples: {len(self.samples)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c1d82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Model -----------\n",
    "\n",
    "class CoordRegressionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 32 * 32, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2)  # x and y\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.regressor(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c7e1de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Scanning: C:\\Users\\luukn\\AIMI_MONKEY2\\monkey-training\\images/pas-original\n",
      "Found 18 image(s)\n",
      "Processing: C:\\Users\\luukn\\AIMI_MONKEY2\\monkey-training\\images/pas-original\\D_P000001_PAS_Original.tif\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TiffPage.asarray() got an unexpected keyword argument 'memmap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device:\u001b[39m\u001b[38;5;124m\"\u001b[39m, device)\n\u001b[1;32m----> 4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mInflammatoryCellDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[21], line 28\u001b[0m, in \u001b[0;36mInflammatoryCellDataset.__init__\u001b[1;34m(self, image_dir_list, annotation_dir, patch_size_px, num_patches)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tifffile\u001b[38;5;241m.\u001b[39mTiffFile(img_path) \u001b[38;5;28;01mas\u001b[39;00m tif:\n\u001b[0;32m     27\u001b[0m     page \u001b[38;5;241m=\u001b[39m tif\u001b[38;5;241m.\u001b[39mpages[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 28\u001b[0m     mm \u001b[38;5;241m=\u001b[39m \u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# memory-mapped numpy array\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     h, w \u001b[38;5;241m=\u001b[39m mm\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     31\u001b[0m     patches_sampled \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: TiffPage.asarray() got an unexpected keyword argument 'memmap'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "dataset = InflammatoryCellDataset(train_dirs, annotation_dir)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "print(\"Loaded dataset\")\n",
    "\n",
    "model = CoordRegressionCNN().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9210406e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Scanning: C:\\Users\\luukn\\AIMI_MONKEY2\\monkey-training\\images/pas-original\n",
      "Found 18 image(s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device:\u001b[39m\u001b[38;5;124m\"\u001b[39m, device)\n\u001b[1;32m----> 6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mInflammatoryCellDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 38\u001b[0m, in \u001b[0;36mInflammatoryCellDataset.__init__\u001b[1;34m(self, image_dir_list, annotation_dir, patch_size, stride)\u001b[0m\n\u001b[0;32m     36\u001b[0m local_targets \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (gx, gy) \u001b[38;5;129;01min\u001b[39;00m ann_coords:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x_offset \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m gx \u001b[38;5;241m<\u001b[39m x_offset \u001b[38;5;241m+\u001b[39m patch_size \u001b[38;5;129;01mand\u001b[39;00m y_offset \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m gy \u001b[38;5;241m<\u001b[39m y_offset \u001b[38;5;241m+\u001b[39m patch_size:\n\u001b[0;32m     39\u001b[0m         local_x \u001b[38;5;241m=\u001b[39m gx \u001b[38;5;241m-\u001b[39m x_offset\n\u001b[0;32m     40\u001b[0m         local_y \u001b[38;5;241m=\u001b[39m gy \u001b[38;5;241m-\u001b[39m y_offset\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------- Training -----------\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    print(f'Model training... Epoch {epochs}')\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for images, targets in loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "        preds = model(images)\n",
    "        loss = criterion(preds, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {total_loss / len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2862392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_pas_cpg(model, image_dir=\"images/pas-cpg\", save_path=\"cpg_predictions.json\"):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    for image_path in glob(os.path.join(image_dir, \"*.tif\")):\n",
    "        print(f\"Inferencing on {image_path}\")\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        H, W = image.shape[:2]\n",
    "\n",
    "        patches, coords = extract_patches(image, stride=STRIDE, patch_size=PATCH_SIZE)\n",
    "        batch = []\n",
    "        batch_origins = []\n",
    "\n",
    "        for patch, (x_offset, y_offset) in zip(patches, coords):\n",
    "            patch_tensor = transform(patch).unsqueeze(0)\n",
    "            batch.append(patch_tensor)\n",
    "            batch_origins.append((x_offset, y_offset))\n",
    "\n",
    "            if len(batch) == 32:\n",
    "                input_batch = torch.cat(batch).to(device)\n",
    "                with torch.no_grad():\n",
    "                    preds = model(input_batch)\n",
    "\n",
    "                for i, pred in enumerate(preds):\n",
    "                    gx = int(pred[0].item() + batch_origins[i][0])\n",
    "                    gy = int(pred[1].item() + batch_origins[i][1])\n",
    "                    results.append({\n",
    "                        \"image_id\": os.path.basename(image_path).split('.')[0],\n",
    "                        \"cell_type\": \"inflammatory-cell\",\n",
    "                        \"coordinates\": [gx, gy]\n",
    "                    })\n",
    "\n",
    "                batch = []\n",
    "                batch_origins = []\n",
    "\n",
    "        # Remaining patches\n",
    "        if batch:\n",
    "            input_batch = torch.cat(batch).to(device)\n",
    "            with torch.no_grad():\n",
    "                preds = model(input_batch)\n",
    "\n",
    "            for i, pred in enumerate(preds):\n",
    "                gx = int(pred[0].item() + batch_origins[i][0])\n",
    "                gy = int(pred[1].item() + batch_origins[i][1])\n",
    "                results.append({\n",
    "                    \"image_id\": os.path.basename(image_path).split('.')[0],\n",
    "                    \"cell_type\": \"inflammatory-cell\",\n",
    "                    \"coordinates\": [gx, gy]\n",
    "                })\n",
    "\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(f\"Saved {len(results)} predictions to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdd651d",
   "metadata": {},
   "source": [
    "## Try 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8554dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from glob import glob\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fc97982",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 256\n",
    "STRIDE = 128\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "def extract_patches(image, stride=STRIDE, patch_size=PATCH_SIZE):\n",
    "    H, W = image.shape[:2]\n",
    "    patches = []\n",
    "    coords = []\n",
    "\n",
    "    for y in range(0, H - patch_size + 1, stride):\n",
    "        for x in range(0, W - patch_size + 1, stride):\n",
    "            patch = image[y:y+patch_size, x:x+patch_size]\n",
    "            patches.append(patch)\n",
    "            coords.append((x, y))\n",
    "    return patches, coords\n",
    "\n",
    "def parse_annotations(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    coords = []\n",
    "    for obj in data.get(\"cells\", []):\n",
    "        x, y = obj[\"x\"], obj[\"y\"]\n",
    "        coords.append((x, y))\n",
    "    return coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbb3655b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scanning: images/pas-original\n",
      "❌ No images found in: images/pas-original\n",
      "\n",
      "Scanning: images/pas-diagnostic\n",
      "❌ No images found in: images/pas-diagnostic\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import json\n",
    "\n",
    "# Root settings\n",
    "train_dirs = [\"images/pas-original\", \"images/pas-diagnostic\"]\n",
    "annotation_dir = \"annotations/json\"  # assuming you're using pixel coords here\n",
    "\n",
    "def check_image_and_annotation_pairs(img_dir, ann_dir):\n",
    "    print(f\"\\nScanning: {img_dir}\")\n",
    "    image_files = glob(os.path.join(img_dir, \"*.tif\"))\n",
    "    if not image_files:\n",
    "        print(\"❌ No images found in:\", img_dir)\n",
    "        return\n",
    "\n",
    "    matched = 0\n",
    "    for img_path in image_files:\n",
    "        filename = os.path.basename(img_path)  # e.g., A_P000001_PAS_Original.tif\n",
    "        slide_id = \"_\".join(filename.split('_')[:2])  # e.g., A_P000001\n",
    "        ann_path = os.path.join(ann_dir, f\"{slide_id}_inflammatory-cells.json\")\n",
    "        \n",
    "        if os.path.exists(ann_path):\n",
    "            matched += 1\n",
    "            try:\n",
    "                with open(ann_path, \"r\") as f:\n",
    "                    ann = json.load(f)\n",
    "                coords = ann.get(\"coordinates\") or ann.get(\"points\") or ann\n",
    "                if isinstance(coords, list) and coords:\n",
    "                    print(f\"✅ {slide_id} → {len(coords)} annotations\")\n",
    "                else:\n",
    "                    print(f\"⚠️  {slide_id} → No annotation points in JSON\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  {slide_id} → Failed to load JSON: {e}\")\n",
    "        else:\n",
    "            print(f\"❌ {slide_id} → Annotation not found at {ann_path}\")\n",
    "    \n",
    "    print(f\"\\nSummary for {img_dir}: {matched} images have matching annotations.\")\n",
    "\n",
    "# Check both train folders\n",
    "for train_dir in train_dirs:\n",
    "    check_image_and_annotation_pairs(train_dir, annotation_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3fc9b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: c:\\Users\\luukn\\OneDrive\\Documenten\\RadboudUniversity\\Master\\Year1\\Q3\\AIMI\\Project\\AIMI_MONKEY\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e972e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InflammatoryCellDataset(Dataset):\n",
    "    def __init__(self, image_dir_list, annotation_dir, patch_size=256, stride=128):\n",
    "        self.samples = []\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        print(image_dir_list)\n",
    "\n",
    "        for img_dir in image_dir_list:\n",
    "            image_paths = os.path.join(img_dir, \"*.tif\")\n",
    "            print(image_paths)\n",
    "            image_files = glob(os.path.join(img_dir, \"*.tif\"))\n",
    "            print(image_files)\n",
    "            for img_path in image_files:\n",
    "                filename = os.path.basename(img_path)  # e.g., A_P000001_PAS_Original.tif\n",
    "                slide_id = \"_\".join(filename.split('_')[:2])  # -> A_P000001\n",
    "                ann_path = os.path.join(annotation_dir, f\"{slide_id}_inflammatory-cells.json\")\n",
    "                print(f\"Processing {img_path} with annotation {ann_path}\")\n",
    "                if not os.path.exists(ann_path):\n",
    "                    print(f\"Annotation file not found for {img_path}: {ann_path}\")\n",
    "                    continue\n",
    "\n",
    "                img = cv2.imread(img_path)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                ann_coords = parse_annotations(ann_path)\n",
    "                patches, coords = extract_patches(img, stride, patch_size)\n",
    "\n",
    "                for patch, (x_offset, y_offset) in zip(patches, coords):\n",
    "                    local_targets = []\n",
    "                    for (gx, gy) in ann_coords:\n",
    "                        if x_offset <= gx < x_offset + patch_size and y_offset <= gy < y_offset + patch_size:\n",
    "                            local_x = gx - x_offset\n",
    "                            local_y = gy - y_offset\n",
    "                            local_targets.append((local_x, local_y))\n",
    "                    if local_targets:\n",
    "                        for t in local_targets:\n",
    "                            self.samples.append((patch, t))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patch, (x, y) = self.samples[idx]\n",
    "        patch = transform(patch)\n",
    "        target = torch.tensor([x, y], dtype=torch.float32)\n",
    "        return patch, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0002a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoordRegressionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, stride=1, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, stride=1, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.regressor = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.regressor(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7f5e197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['images/pas-original', 'images/pas-diagnostic']\n",
      "images/pas-original\\*.tif\n",
      "[]\n",
      "images/pas-diagnostic\\*.tif\n",
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m train_dirs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages/pas-original\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages/pas-diagnostic\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m InflammatoryCellDataset(train_dirs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotations/json_pixel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m CoordRegressionCNN()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:376\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 376\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:164\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    166\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_dirs = [\"images/pas-original\", \"images/pas-diagnostic\"]\n",
    "dataset = InflammatoryCellDataset(train_dirs, \"annotations/json_pixel\")\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = CoordRegressionCNN().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "EPOCHS = 1\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for images, targets in loader:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        preds = model(images)\n",
    "        loss = criterion(preds, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8fb95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_pas_cpg(model, image_dir=\"images/pas-cpg\", save_path=\"cpg_predictions.json\"):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    for image_path in glob(os.path.join(image_dir, \"*.tif\")):\n",
    "        print(f\"Inferencing on {image_path}\")\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        H, W = image.shape[:2]\n",
    "\n",
    "        patches, coords = extract_patches(image, stride=STRIDE, patch_size=PATCH_SIZE)\n",
    "        batch = []\n",
    "        batch_origins = []\n",
    "\n",
    "        for patch, (x_offset, y_offset) in zip(patches, coords):\n",
    "            patch_tensor = transform(patch).unsqueeze(0)\n",
    "            batch.append(patch_tensor)\n",
    "            batch_origins.append((x_offset, y_offset))\n",
    "\n",
    "            if len(batch) == 32:\n",
    "                input_batch = torch.cat(batch).to(device)\n",
    "                with torch.no_grad():\n",
    "                    preds = model(input_batch)\n",
    "\n",
    "                for i, pred in enumerate(preds):\n",
    "                    gx = int(pred[0].item() + batch_origins[i][0])\n",
    "                    gy = int(pred[1].item() + batch_origins[i][1])\n",
    "                    results.append({\n",
    "                        \"image_id\": os.path.basename(image_path).split('.')[0],\n",
    "                        \"cell_type\": \"inflammatory-cell\",\n",
    "                        \"coordinates\": [gx, gy]\n",
    "                    })\n",
    "\n",
    "                batch = []\n",
    "                batch_origins = []\n",
    "\n",
    "        # Remaining patches\n",
    "        if batch:\n",
    "            input_batch = torch.cat(batch).to(device)\n",
    "            with torch.no_grad():\n",
    "                preds = model(input_batch)\n",
    "\n",
    "            for i, pred in enumerate(preds):\n",
    "                gx = int(pred[0].item() + batch_origins[i][0])\n",
    "                gy = int(pred[1].item() + batch_origins[i][1])\n",
    "                results.append({\n",
    "                    \"image_id\": os.path.basename(image_path).split('.')[0],\n",
    "                    \"cell_type\": \"inflammatory-cell\",\n",
    "                    \"coordinates\": [gx, gy]\n",
    "                })\n",
    "\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(f\"Saved {len(results)} predictions to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae4b064",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_on_pas_cpg(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6262dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save(model.state_dict(), \"inflammatory_model.pth\")\n",
    "\n",
    "# Load later\n",
    "model = CoordRegressionCNN()\n",
    "model.load_state_dict(torch.load(\"inflammatory_model.pth\"))\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
