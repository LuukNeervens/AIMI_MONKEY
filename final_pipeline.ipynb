{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae0fbe45",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626a0f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import random\n",
    "import shutil\n",
    "import copy\n",
    "#from kaggle_secrets import UserSecretsClient\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "VERBOSE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba3cfea",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf122d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InflammatoryCellsDataset(Dataset):\n",
    "    def __init__(self, root_dir, annotations_dir, splits, transforms=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.annotations_dir = Path(annotations_dir)\n",
    "        self.splits = splits\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.image_paths = []\n",
    "        self.annotation_paths = []\n",
    "\n",
    "        for split in splits:\n",
    "            image_dir = self.root_dir / split / \"images\"\n",
    "            ann_dir = self.annotations_dir / split / \"annotations\"\n",
    "\n",
    "            for image_path in image_dir.glob(\"*.png\"):\n",
    "                ann_path = ann_dir / image_path.name.replace(\".png\", \".json\")\n",
    "                if ann_path.exists():\n",
    "                    # Load the annotation and check for non-empty bbox list\n",
    "                    with open(ann_path) as f:\n",
    "                        ann_data = json.load(f)\n",
    "                    if not ann_data:  # skip empty annotation files\n",
    "                        continue\n",
    "                        \n",
    "                    self.image_paths.append(image_path)\n",
    "                    self.annotation_paths.append(ann_path)\n",
    "\n",
    "        assert len(self.image_paths) == len(self.annotation_paths), \"Mismatch in images and annotations\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.image_paths[idx]\n",
    "        ann_path = self.annotation_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Load annotations\n",
    "        with open(ann_path) as f:\n",
    "            ann_data = json.load(f)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for item in ann_data:\n",
    "            boxes.append(item[\"bbox\"])\n",
    "            labels.append(1)  # Binary label: 1 = inflammatory cell\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([idx])\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float32))\n",
    "    if train:\n",
    "        # add data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "\n",
    "def get_model(num_classes=2):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "# subsample train, val, and test datasets for given fraction\n",
    "def subsample_dataset(dataset, fraction):\n",
    "    indices = torch.randperm(len(dataset))[:int(len(dataset) * fraction)]\n",
    "    return torch.utils.data.Subset(dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e8e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_iou(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    Compute IoU between two sets of boxes.\n",
    "    boxes1, boxes2: [N,4] tensors (xmin, ymin, xmax, ymax)\n",
    "    Returns IoU matrix [N, M]\n",
    "    \"\"\"\n",
    "    area1 = (boxes1[:, 2] - boxes1[:, 0]).clamp(min=0) * (boxes1[:, 3] - boxes1[:, 1]).clamp(min=0)\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]).clamp(min=0) * (boxes2[:, 3] - boxes2[:, 1]).clamp(min=0)\n",
    "\n",
    "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
    "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
    "\n",
    "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
    "\n",
    "    union = area1[:, None] + area2 - inter\n",
    "    iou = inter / union\n",
    "    return iou\n",
    "\n",
    "def evaluate_precision_recall(outputs, targets, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute TP, FP, FN for a batch of predictions and targets.\n",
    "    outputs: list of dicts with keys ['boxes', 'labels', 'scores']\n",
    "    targets: list of dicts with keys ['boxes', 'labels']\n",
    "    \"\"\"\n",
    "\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "\n",
    "    for preds, targs in zip(outputs, targets):\n",
    "        pred_boxes = preds['boxes']\n",
    "        pred_scores = preds['scores']\n",
    "        pred_labels = preds['labels']\n",
    "\n",
    "        true_boxes = targs['boxes']\n",
    "        true_labels = targs['labels']\n",
    "\n",
    "        if len(pred_boxes) == 0:\n",
    "            # No predictions, all true boxes are false negatives\n",
    "            FN += len(true_boxes)\n",
    "            continue\n",
    "\n",
    "        if len(true_boxes) == 0:\n",
    "            # No ground truth, all preds are false positives\n",
    "            FP += len(pred_boxes)\n",
    "            continue\n",
    "\n",
    "        # Filter predictions by score threshold (optional, e.g., 0.5)\n",
    "        score_thresh = 0.5\n",
    "        keep = pred_scores >= score_thresh\n",
    "        pred_boxes = pred_boxes[keep]\n",
    "        pred_labels = pred_labels[keep]\n",
    "\n",
    "        if len(pred_boxes) == 0:\n",
    "            FN += len(true_boxes)\n",
    "            continue\n",
    "\n",
    "        # Compute IoU matrix between preds and true boxes\n",
    "        ious = box_iou(pred_boxes, true_boxes)  # [num_pred, num_true]\n",
    "\n",
    "        # Match preds to true boxes by IoU > threshold and label match\n",
    "        matched_gt = set()\n",
    "        matched_pred = set()\n",
    "\n",
    "        for pred_idx in range(ious.shape[0]):\n",
    "            for gt_idx in range(ious.shape[1]):\n",
    "                if ious[pred_idx, gt_idx] >= iou_threshold and pred_labels[pred_idx] == true_labels[gt_idx]:\n",
    "                    if gt_idx not in matched_gt and pred_idx not in matched_pred:\n",
    "                        matched_gt.add(gt_idx)\n",
    "                        matched_pred.add(pred_idx)\n",
    "\n",
    "        TP += len(matched_pred)\n",
    "        FP += len(pred_boxes) - len(matched_pred)\n",
    "        FN += len(true_boxes) - len(matched_gt)\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779da16b",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f51610",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/kaggle/input/patches-with-annotations/patches_with_annotations\"\n",
    "test_dir = \"/kaggle/input/patches/patches_newest\"\n",
    "annotations_dir = \"/kaggle/input/annotations/json_mm\"\n",
    "\n",
    "# Prepare combined train splits\n",
    "train_splits = ['pas-original', 'pas-diagnostic']\n",
    "full_train = InflammatoryCellsDataset(root_dir, root_dir, splits=train_splits, transforms=get_transform(True))\n",
    "# 80/20 train/val split\n",
    "train_size = int(0.8 * len(full_train))\n",
    "val_size = len(full_train) - train_size\n",
    "train_ds, val_ds = random_split(full_train, [train_size, val_size])\n",
    "\n",
    "# PAS-CPG test set\n",
    "test_ds = InflammatoryCellsDataset(root_dir, annotations_dir, splits=['cpg_test'], transforms=get_transform(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ff0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction = 1  # 100% of the dataset\n",
    "train_ds = subsample_dataset(train_ds, fraction)  # 10% of training data\n",
    "val_ds = subsample_dataset(val_ds, fraction)    # 10% of validation data\n",
    "test_ds = subsample_dataset(test_ds, fraction)  # 10% of test data\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = get_model(num_classes=2)\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "if VERBOSE:\n",
    "    print(f\"Training on {len(train_ds)} samples (80% split from: {train_splits}\")\n",
    "    print(f\"Validating on {len(val_ds)} samples (20% split from: {train_splits})\")\n",
    "    #print(f\"Evaluating on {len(test_ds)} samples from split: pas-cpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff5502b",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8e0651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, train_loader, val_loader, device, num_epochs=10, lr_scheduler=None):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_map = 0.0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Record training history\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_precision\": [],\n",
    "        \"val_recall\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "        # ---------- Training ----------\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        for images, targets in tqdm(train_loader, desc=\"Training\"):\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            total_loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += total_loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        history[\"train_loss\"].append(avg_train_loss)\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        if lr_scheduler:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        # ---------- Validation ----------\n",
    "        model.eval()\n",
    "        val_precision = 0.0\n",
    "        val_recall = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images = list(img.to(device) for img in images)\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "                outputs = model(images)  # list of dicts\n",
    "        \n",
    "                precision, recall = evaluate_precision_recall(outputs, targets)\n",
    "                val_precision += precision\n",
    "                val_recall += recall\n",
    "                num_batches += 1\n",
    "        \n",
    "        avg_precision = val_precision / num_batches\n",
    "        avg_recall = val_recall / num_batches\n",
    "        history[\"val_precision\"].append(avg_precision)\n",
    "        history[\"val_recall\"].append(avg_recall)\n",
    "        \n",
    "        print(f\"Validation Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f}\")\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d85706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional LR scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "model, history = train_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    num_epochs=10,\n",
    "    lr_scheduler=lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d67e24",
   "metadata": {},
   "source": [
    "### Automatically saving to public dataset (for Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e0890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup Kaggle API token from input dataset (uploaded file)\n",
    "# os.makedirs(\"/root/.config/kaggle\", exist_ok=True)\n",
    "# shutil.copy(\"/kaggle/input/kagglekey/kaggle.json\", \"/root/.config/kaggle/kaggle.json\")\n",
    "# os.chmod(\"/root/.config/kaggle/kaggle.json\", 0o600)\n",
    "# print(\"✅ Kaggle API token configured.\")\n",
    "\n",
    "# # Save your model + training history together\n",
    "# output_dir = \"saved_model_new\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# model_path = os.path.join(output_dir, \"FasterRCNN_test.pth\")\n",
    "\n",
    "# torch.save({\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#     'history': history\n",
    "# }, model_path)\n",
    "\n",
    "# print(f\"Model + history saved to: {model_path}\")\n",
    "\n",
    "# # Create metadata file for Kaggle dataset\n",
    "# dataset_name = \"AIMI-project\"\n",
    "# username = \"luukneervens\"\n",
    "\n",
    "# metadata = {\n",
    "#     \"title\": dataset_name,\n",
    "#     \"id\": f\"{username}/{dataset_name}\",\n",
    "#     \"licenses\": [{\"name\": \"CC0-1.0\"}]\n",
    "# }\n",
    "\n",
    "# with open(os.path.join(output_dir, \"dataset-metadata.json\"), \"w\") as f:\n",
    "#     json.dump(metadata, f)\n",
    "\n",
    "# # Upload or update dataset on Kaggle\n",
    "# !kaggle datasets create -p saved_model_new -u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd911034",
   "metadata": {},
   "source": [
    "### Plot training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f383ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 5))\n",
    "# plt.plot(history['train_loss'], label='Train Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training Loss Over Epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524ca3fe",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32199047",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = get_model(num_classes=2)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"/kaggle/input/fasterrcnn1/pytorch/default/1/FasterRCNN1.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f59462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interW = max(0, xB - xA)\n",
    "    interH = max(0, yB - yA)\n",
    "    interArea = interW * interH\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    unionArea = boxAArea + boxBArea - interArea\n",
    "    return interArea / unionArea if unionArea > 0 else 0\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "def visualize_and_save(img: Image.Image, gt_boxes, pred_boxes, \n",
    "                       out_path: str, \n",
    "                       gt_color=(0,255,0), pred_color=(255,0,0), \n",
    "                       thickness=2):\n",
    "    \"\"\"\n",
    "    Draws gt_boxes and pred_boxes onto img and saves to out_path.\n",
    "    gt_boxes, pred_boxes: lists of [xmin,ymin,xmax,ymax]\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Optional: load a default font for labels\n",
    "    try:\n",
    "        font = ImageFont.load_default()\n",
    "    except:\n",
    "        font = None\n",
    "\n",
    "    # Draw GT boxes\n",
    "    for box in gt_boxes:\n",
    "        draw.rectangle(box, outline=gt_color, width=thickness)\n",
    "        if font:\n",
    "            draw.text((box[0], box[1]-10), \"GT\", fill=gt_color, font=font)\n",
    "\n",
    "    # Draw Predicted boxes\n",
    "    for box in pred_boxes:\n",
    "        draw.rectangle(box, outline=pred_color, width=thickness)\n",
    "        if font:\n",
    "            draw.text((box[0], box[3]+2), \"PRED\", fill=pred_color, font=font)\n",
    "\n",
    "    img.save(out_path)\n",
    "\n",
    "def evaluate_on_patches(model, images_dir, anns_dir, output_vis_dir, iou_threshold=0.5, score_thresh=0.5, device=\"cuda\", fraction=0.5):\n",
    "    model.to(device).eval()\n",
    "\n",
    "    TP = FP = FN = 0\n",
    "    image_paths = glob.glob(os.path.join(Path(images_dir), \"*.png\"))\n",
    "    print(f\"Found {len(image_paths)} patches, testing on random subset of {int(fraction*len(image_paths))} patches\")\n",
    "\n",
    "    # take random subset of all test patches\n",
    "    random.shuffle(image_paths)\n",
    "    image_paths = image_paths[:int(len(image_paths) * fraction)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_path in tqdm(image_paths):\n",
    "            patient_id = '_'.join(Path(img_path).stem.split('_')[:2])\n",
    "            #print(f'processing image: {patient_id}...')\n",
    "\n",
    "            # 1) Load and preprocess\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img_t = to_tensor(img).to(device)            # C×H×W tensor in [0,1]\n",
    "            preds = model([img_t])[0]                   # dict with 'boxes', 'scores', 'labels'\n",
    "\n",
    "            # 2) Filter out low‐confidence preds\n",
    "            keep = preds[\"scores\"] > score_thresh\n",
    "            pred_boxes = preds[\"boxes\"][keep].cpu().tolist()\n",
    "\n",
    "            # 3) Load GT boxes\n",
    "            stem = Path(img_path).stem\n",
    "            ann_path = Path(anns_dir) / f\"{stem}.json\"\n",
    "            with open(ann_path, 'r') as f:\n",
    "                gt_data = json.load(f)\n",
    "            if isinstance(gt_data, dict) and \"annotations\" in gt_data:\n",
    "                gt_boxes = [ann[\"bbox\"] for ann in gt_data[\"annotations\"]]\n",
    "            else:\n",
    "                gt_boxes = [ann[\"bbox\"] for ann in gt_data]\n",
    "\n",
    "            # 4) Match preds → GT\n",
    "            matched_gt = set()\n",
    "            for pb in pred_boxes:\n",
    "                best_iou, best_j = 0, -1\n",
    "                for j, gb in enumerate(gt_boxes):\n",
    "                    if j in matched_gt: \n",
    "                        continue\n",
    "                    score = iou(pb, gb)\n",
    "                    if score > best_iou:\n",
    "                        best_iou, best_j = score, j\n",
    "\n",
    "                if best_iou >= iou_threshold:\n",
    "                    TP += 1\n",
    "                    matched_gt.add(best_j)\n",
    "                else:\n",
    "                    FP += 1\n",
    "\n",
    "            # 5) Any unmatched GT → FN\n",
    "            FN += (len(gt_boxes) - len(matched_gt))\n",
    "\n",
    "            out_file = os.path.join(output_vis_dir, f\"{stem}_vis.png\")\n",
    "            visualize_and_save(\n",
    "                img.copy(),        # copy so we don’t alter img in memory\n",
    "                gt_boxes, \n",
    "                pred_boxes, \n",
    "                out_file\n",
    "            )\n",
    "\n",
    "    # 6) Compute metrics\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0.0\n",
    "    recall    = TP / (TP + FN) if TP + FN > 0 else 0.0\n",
    "    f1_score  = 2 * (precision * recall) / (precision + recall) \\\n",
    "                    if (precision + recall) else 0.0\n",
    "\n",
    "    print(f\"--- Evaluation over {len(image_paths)} patches ---\")\n",
    "    print(f\"TP: {TP}, FP: {FP}, FN: {FN}\")\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score : {f1_score:.4f}\")\n",
    "\n",
    "    return {\"TP\": TP, \"FP\": FP, \"FN\": FN, \"precision\": precision, \"recall\": recall, \"f1\": f1_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ecf848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to the correct absolute Kaggle paths\n",
    "images_dir = \"/kaggle/input/test-patches-with-annotations/test_patches_with_annotations/cpg/images\"\n",
    "anns_dir   = \"/kaggle/input/test-patches-with-annotations/test_patches_with_annotations/cpg/annotations\"\n",
    "output_vis_dir = \"detection_visuals\"\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "metrics = evaluate_on_patches(\n",
    "    model,\n",
    "    images_dir,\n",
    "    anns_dir,\n",
    "    output_vis_dir,\n",
    "    iou_threshold=0.5,\n",
    "    score_thresh=0.5,\n",
    "    device=device,  # or \"cpu\" if you prefer\n",
    "    fraction=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a2650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip test detection output images for download (for Kaggle)\n",
    "#shutil.make_archive(\"detection_visuals1.0\", 'zip', \"detection_visuals\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
