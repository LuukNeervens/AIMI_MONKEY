{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a3006a2",
   "metadata": {},
   "source": [
    "# Try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6757544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import openslide\n",
    "from torchvision import transforms as T\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import tifffile\n",
    "\n",
    "from openslide import OpenSlideError\n",
    "\n",
    "\n",
    "from torchvision.models.detection.ssdlite import ssdlite320_mobilenet_v3_large\n",
    "from torchvision.models.detection.ssdlite import SSDLiteClassificationHead\n",
    "from torchvision.models.mobilenet import MobileNet_V3_Large_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05866ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: load XML point annotations from ASAP-style XML (Dot annotations)\n",
    "def load_xml_annotations(xml_path):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    boxes, labels = [], []\n",
    "    # annotations under /ASAP_Annotations/Annotations/Annotation\n",
    "    for ann in root.findall('.//Annotation'):\n",
    "        coords = ann.find('Coordinates/Coordinate')\n",
    "        x = float(coords.get('X'))\n",
    "        y = float(coords.get('Y'))\n",
    "        # create tiny box around point (e.g. 1x1 pixel)\n",
    "        boxes.append([x, y, x+1.0, y+1.0])\n",
    "        labels.append(1)\n",
    "    return torch.tensor(boxes, dtype=torch.float32), torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "\n",
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, img_dir, ann_dir, mask_dir=None, patch_size=512, transforms=None):\n",
    "        self.img_paths = sorted(glob.glob(os.path.join(img_dir, '*.tif')))\n",
    "        self.ann_paths = []\n",
    "        for img_path in self.img_paths:\n",
    "            base = os.path.splitext(os.path.basename(img_path))[0].split('_')[:2]  # Take only C_P000031\n",
    "            base = '_'.join(base)\n",
    "            match = os.path.join(ann_dir, f\"{base}.xml\")\n",
    "            if os.path.exists(match):\n",
    "                self.ann_paths.append(match)\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Annotation XML for slide {base} not found in {ann_dir}\")\n",
    "        self.mask_paths = ([os.path.join(mask_dir, os.path.basename(p)) for p in self.img_paths] if mask_dir else [None]*len(self.img_paths))\n",
    "        self.patch_size = patch_size\n",
    "        self.transforms = transforms or T.Compose([T.Resize((320,320)), T.ToTensor()])\n",
    "                # Precompute ROI bounding boxes from binary mask (values > 0 are ROI)\n",
    "        self.rois = []\n",
    "        for mask_path in self.mask_paths:\n",
    "            if mask_path and os.path.exists(mask_path):\n",
    "                mask_arr = tifffile.imread(mask_path)\n",
    "                # treat any non-zero pixel as ROI\n",
    "                mask = mask_arr > 0\n",
    "                coords = np.argwhere(mask)\n",
    "                if coords.size > 0:\n",
    "                    y0, x0 = coords.min(axis=0)\n",
    "                    y1, x1 = coords.max(axis=0)\n",
    "                    self.rois.append((x0, y0, x1, y1))\n",
    "                else:\n",
    "                    self.rois.append(None)\n",
    "            else:\n",
    "                self.rois.append(None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        slide_path = self.img_paths[idx]\n",
    "        slide = openslide.OpenSlide(slide_path)\n",
    "        w0, h0 = slide.level_dimensions[0]\n",
    "        roi = self.rois[idx] or (0, 0, w0, h0)\n",
    "        x0, y0, x1, y1 = roi\n",
    "        # clamp ROI within slide bounds\n",
    "        x1 = min(x1, w0); y1 = min(y1, h0)\n",
    "        w_roi, h_roi = x1 - x0, y1 - y0\n",
    "        try:\n",
    "            region = slide.read_region((x0, y0), 0, (w_roi, h_roi)).convert('RGB')\n",
    "        except OpenSlideError:\n",
    "            # fallback: read a smaller region around center\n",
    "            cx = w0 // 2; cy = h0 // 2\n",
    "            half = self.patch_size // 2\n",
    "            x0 = max(cx - half, 0); y0 = max(cy - half, 0)\n",
    "            region = slide.read_region((x0, y0), 0, (self.patch_size, self.patch_size)).convert('RGB')\n",
    "            boxes, labels = load_xml_annotations(self.ann_paths[idx])\n",
    "            return [self.transforms(region)], [{'boxes': torch.tensor([[0,0,self.patch_size,self.patch_size]]), 'labels': torch.tensor([1])}]\n",
    "        # Load annotations and adjust\n",
    "        boxes, labels = load_xml_annotations(self.ann_paths[idx])\n",
    "        if roi != (0,0,w0,h0):\n",
    "            boxes -= torch.tensor([x0, y0, x0, y0], dtype=torch.float32)\n",
    "        # Sample patches\n",
    "        patches, targets = [], []\n",
    "        half = self.patch_size // 2\n",
    "        for box, label in zip(boxes, labels):\n",
    "            cx = int((box[0] + box[2]) / 2); cy = int((box[1] + box[3]) / 2)\n",
    "            x_start = np.clip(cx - half, 0, w_roi - self.patch_size)\n",
    "            y_start = np.clip(cy - half, 0, h_roi - self.patch_size)\n",
    "            patch = region.crop((x_start, y_start, x_start + self.patch_size, y_start + self.patch_size))\n",
    "            tensor = self.transforms(patch)\n",
    "            adj = box - torch.tensor([x_start, y_start, x_start, y_start], dtype=torch.float32)\n",
    "            patches.append(tensor)\n",
    "            targets.append({'boxes': adj.unsqueeze(0), 'labels': label.unsqueeze(0)})\n",
    "        return patches, targets\n",
    "    \n",
    "\n",
    "# Custom collate to flatten\n",
    "def collate_fn(batch):\n",
    "    imgs, targets = [], []\n",
    "    for patch_list, targ_list in batch:\n",
    "        imgs.extend(patch_list)\n",
    "        targets.extend(targ_list)\n",
    "    return imgs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dd4349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dir = \"C:/Users/luukn/AIMI_MONKEY2/monkey-training/images/pas-original\"\n",
    "diag_dir = \"C:/Users/luukn/AIMI_MONKEY2/monkey-training/images/pas-diagnostic\"\n",
    "cpg_dir  = \"C:/Users/luukn/AIMI_MONKEY2/monkey-training/images/pas-cpg\"\n",
    "ann_dir  = \"C:/Users/luukn/AIMI_MONKEY2/monkey-training/annotations/xml\"\n",
    "mask_dir = \"C:/Users/luukn/AIMI_MONKEY2/monkey-training/images/tissue-masks\"\n",
    "\n",
    "# Build datasets and loaders\n",
    "full_ds = torch.utils.data.ConcatDataset([\n",
    "    PatchDataset(orig_dir, ann_dir, mask_dir),\n",
    "    PatchDataset(diag_dir, ann_dir, mask_dir)\n",
    "])\n",
    "val_size = int(0.2 * len(full_ds))\n",
    "train_size = len(full_ds) - val_size\n",
    "train_ds, val_ds = random_split(full_ds, [train_size, val_size])\n",
    "test_ds = PatchDataset(cpg_dir, ann_dir, None)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=2, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19d6d388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "SSD                                                          [133, 4]                  --\n",
       "├─GeneralizedRCNNTransform: 1-1                              [4, 3, 320, 320]          --\n",
       "├─SSDLiteFeatureExtractorMobileNet: 1-2                      [4, 128, 1, 1]            --\n",
       "│    └─Sequential: 2-1                                       --                        --\n",
       "│    │    └─Sequential: 3-1                                  [4, 672, 20, 20]          869,096\n",
       "│    │    └─Sequential: 3-2                                  [4, 960, 10, 10]          2,102,856\n",
       "│    └─ModuleList: 2-2                                       --                        --\n",
       "│    │    └─Sequential: 3-3                                  [4, 512, 5, 5]            381,184\n",
       "│    │    └─Sequential: 3-4                                  [4, 256, 3, 3]            100,480\n",
       "│    │    └─Sequential: 3-5                                  [4, 256, 2, 2]            67,712\n",
       "│    │    └─Sequential: 3-6                                  [4, 128, 1, 1]            25,664\n",
       "├─SSDLiteHead: 1-3                                           [4, 3234, 2]              --\n",
       "│    └─SSDLiteRegressionHead: 2-3                            [4, 3234, 4]              --\n",
       "│    │    └─ModuleList: 3-7                                  --                        97,584\n",
       "│    └─SSDLiteClassificationHead: 2-4                        [4, 3234, 2]              --\n",
       "│    │    └─ModuleList: 3-8                                  --                        64,104\n",
       "├─DefaultBoxGenerator: 1-4                                   [3234, 4]                 --\n",
       "==============================================================================================================\n",
       "Total params: 3,708,680\n",
       "Trainable params: 3,708,680\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.95\n",
       "==============================================================================================================\n",
       "Input size (MB): 4.92\n",
       "Forward/backward pass size (MB): 627.72\n",
       "Params size (MB): 14.83\n",
       "Estimated Total Size (MB): 647.47\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build SSD-Lite MobileNetV3 model for MNL detection\n",
    "def get_model(num_classes=2):\n",
    "    # use pretrained MobilenetV3 backbone, random head\n",
    "    model = ssdlite320_mobilenet_v3_large(\n",
    "        weights=None,\n",
    "        weights_backbone=MobileNet_V3_Large_Weights.IMAGENET1K_V2,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    return model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = get_model(num_classes=2).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Print summary\n",
    "summary(model, input_size=[(4, 3, 320, 320)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af17625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01bcacc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenSlideError",
     "evalue": "Cannot read raw tile",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenSlideError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 62\u001b[0m, in \u001b[0;36mPatchDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     region \u001b[38;5;241m=\u001b[39m \u001b[43mslide\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_region\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mw_roi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_roi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OpenSlideError:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# fallback: read a smaller region around center\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openslide\\__init__.py:281\u001b[0m, in \u001b[0;36mOpenSlide.read_region\u001b[1;34m(self, location, level, size)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a PIL.Image containing the contents of the region.\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03mlocation: (x, y) tuple giving the top left pixel in the level 0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03mUnlike in the C interface, the image data returned by this\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;124;03mfunction is not premultiplied.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 281\u001b[0m region \u001b[38;5;241m=\u001b[39m \u001b[43mlowlevel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_region\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_osr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openslide\\lowlevel.py:443\u001b[0m, in \u001b[0;36mread_region\u001b[1;34m(slide, x, y, level, w, h)\u001b[0m\n\u001b[0;32m    442\u001b[0m buf \u001b[38;5;241m=\u001b[39m (w \u001b[38;5;241m*\u001b[39m h \u001b[38;5;241m*\u001b[39m c_uint32)()\n\u001b[1;32m--> 443\u001b[0m \u001b[43m_read_region\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _load_image(buf, (w, h))\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openslide\\lowlevel.py:289\u001b[0m, in \u001b[0;36m_check_error\u001b[1;34m(result, func, args)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenSlideError(err)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _check_string(result, func, args)\n",
      "\u001b[1;31mOpenSlideError\u001b[0m: Cannot read raw tile",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOpenSlideError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      4\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[1;32m----> 5\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSTART BATCH: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgs_resized\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m320\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m320\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:350\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulative_sizes[dataset_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 68\u001b[0m, in \u001b[0;36mPatchDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     66\u001b[0m half \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     67\u001b[0m x0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(cx \u001b[38;5;241m-\u001b[39m half, \u001b[38;5;241m0\u001b[39m); y0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(cy \u001b[38;5;241m-\u001b[39m half, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m region \u001b[38;5;241m=\u001b[39m \u001b[43mslide\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_region\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     69\u001b[0m boxes, labels \u001b[38;5;241m=\u001b[39m load_xml_annotations(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mann_paths[idx])\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms(region)], [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size]]), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m])}]\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openslide\\__init__.py:281\u001b[0m, in \u001b[0;36mOpenSlide.read_region\u001b[1;34m(self, location, level, size)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_region\u001b[39m(\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m, location: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m], level: \u001b[38;5;28mint\u001b[39m, size: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]\n\u001b[0;32m    271\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[0;32m    272\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a PIL.Image containing the contents of the region.\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m    location: (x, y) tuple giving the top left pixel in the level 0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03m    Unlike in the C interface, the image data returned by this\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;124;03m    function is not premultiplied.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 281\u001b[0m     region \u001b[38;5;241m=\u001b[39m \u001b[43mlowlevel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_region\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_osr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    285\u001b[0m         region\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124micc_profile\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openslide\\lowlevel.py:443\u001b[0m, in \u001b[0;36mread_region\u001b[1;34m(slide, x, y, level, w, h)\u001b[0m\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGBA\u001b[39m\u001b[38;5;124m'\u001b[39m, (w, h))\n\u001b[0;32m    442\u001b[0m buf \u001b[38;5;241m=\u001b[39m (w \u001b[38;5;241m*\u001b[39m h \u001b[38;5;241m*\u001b[39m c_uint32)()\n\u001b[1;32m--> 443\u001b[0m \u001b[43m_read_region\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _load_image(buf, (w, h))\n",
      "File \u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openslide\\lowlevel.py:289\u001b[0m, in \u001b[0;36m_check_error\u001b[1;34m(result, func, args)\u001b[0m\n\u001b[0;32m    287\u001b[0m err \u001b[38;5;241m=\u001b[39m get_error(args[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenSlideError(err)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _check_string(result, func, args)\n",
      "\u001b[1;31mOpenSlideError\u001b[0m: Cannot read raw tile"
     ]
    }
   ],
   "source": [
    "for epoch in range(NR_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch = 0 \n",
    "    for imgs, targets in train_loader:\n",
    "        print('START BATCH: ', batch)\n",
    "        imgs_resized = [T.Resize((320, 320))(img).to(device) for img in imgs]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(imgs_resized, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += losses.item()\n",
    "        print(f\"Epoch {epoch+1}, Batch {batch}, Loss: {losses.item():.4f}\")\n",
    "    print('---------------------------------------------------------------------------')\n",
    "    print(f\"Epoch {epoch+1:02d}, Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cfc99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    for imgs, targets in loader:\n",
    "        imgs_resized = [T.Resize((320, 320))(img).to(device) for img in imgs]\n",
    "        outputs = model(imgs_resized)\n",
    "        all_predictions.extend([{k: v.cpu() for k, v in out.items()} for out in outputs])\n",
    "    return all_predictions\n",
    "\n",
    "# Evaluate on test\n",
    "preds = evaluate(model, test_loader, device)\n",
    "torch.save(preds, 'predictions_cpg_ssd.pt')\n",
    "print(\"Saved SSD predictions_cpg_ssd.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574b770",
   "metadata": {},
   "source": [
    "# Try 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eceeaa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "# Allow loading of very large images\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "import tifffile\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision.models.detection.ssdlite import ssdlite320_mobilenet_v3_large\n",
    "from torchvision.models.detection.ssdlite import SSDLiteClassificationHead\n",
    "from torchvision.models.mobilenet import MobileNet_V3_Large_Weights\n",
    "from torchvision import transforms as T\n",
    "from torchinfo import summary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de41fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: load XML annotations\n",
    "def load_xml_annotations(xml_path):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    boxes, labels = [], []\n",
    "    for obj in root.findall('object'):\n",
    "        name = obj.find('name').text\n",
    "        b = obj.find('bndbox')\n",
    "        xmin, ymin = float(b.find('xmin').text), float(b.find('ymin').text)\n",
    "        xmax, ymax = float(b.find('xmax').text), float(b.find('ymax').text)\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        labels.append(1)\n",
    "    return torch.tensor(boxes, dtype=torch.float32), torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "def crop_tissue(img_np, padding=10, bg_threshold=240):\n",
    "    # img_np: HxWxC uint8\n",
    "    gray = np.mean(img_np, axis=2)\n",
    "    mask = gray < bg_threshold\n",
    "    coords = np.argwhere(mask)\n",
    "    if coords.size == 0:\n",
    "        return img_np\n",
    "    y0, x0 = coords.min(axis=0)\n",
    "    y1, x1 = coords.max(axis=0)\n",
    "    # pad and clip\n",
    "    y0 = max(y0 - padding, 0)\n",
    "    x0 = max(x0 - padding, 0)\n",
    "    y1 = min(y1 + padding, img_np.shape[0])\n",
    "    x1 = min(x1 + padding, img_np.shape[1])\n",
    "    return img_np[y0:y1, x0:x1]\n",
    "\n",
    "class MNLDataset(Dataset):\n",
    "    def __init__(self, img_dir, ann_dir, transforms=None, crop_whitespace=False):\n",
    "        self.img_paths = sorted(glob.glob(os.path.join(img_dir, '*.tif')))\n",
    "        self.ann_paths = [\n",
    "            os.path.join(ann_dir, os.path.basename(p).replace('.tif', '.xml'))\n",
    "            for p in self.img_paths\n",
    "        ]\n",
    "        self.transforms = transforms\n",
    "        self.crop_whitespace = crop_whitespace\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_np = tifffile.imread(self.img_paths[idx])\n",
    "        # ensure RGB\n",
    "        if img_np.ndim == 2:\n",
    "            img_np = np.stack([img_np] * 3, axis=-1)\n",
    "        elif img_np.shape[2] > 3:\n",
    "            img_np = img_np[:, :, :3]\n",
    "        # crop\n",
    "        if self.crop_whitespace:\n",
    "            img_np = crop_tissue(img_np)\n",
    "        img = T.ToTensor()(img_np)\n",
    "\n",
    "        boxes, labels = load_xml_annotations(self.ann_paths[idx])\n",
    "        # adjust boxes if cropped\n",
    "        if self.crop_whitespace:\n",
    "            h_off, w_off = np.argwhere(np.mean(img_np, axis=2) < 240).min(axis=0)\n",
    "            boxes -= torch.tensor([w_off, h_off, w_off, h_off])\n",
    "            # clamp\n",
    "            boxes[:, [0,2]] = boxes[:, [0,2]].clamp(0, img.shape[2])\n",
    "            boxes[:, [1,3]] = boxes[:, [1,3]].clamp(0, img.shape[1])\n",
    "        target = {'boxes': boxes, 'labels': labels}\n",
    "\n",
    "        if self.transforms:\n",
    "            img, target = self.transforms(img, target)\n",
    "        return img, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ad986d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets and loaders\n",
    "orig_dir = \"C:/Users/luukn/AIMI_MONKEY2/monkey-training/images/pas-original\"\n",
    "diag_dir = \"C:/Users/luukn/AIMI_MONKEY2/monkey-training/images/pas-diagnostic\"\n",
    "cpg_dir  = \"C:/Users/luukn/AIMI_MONKEY2/monkey-training/images/pas-cpg\"\n",
    "ann_dir  = \"C:/Users/luukn/AIMI_MONKEY2/monkey-training/annotations/xml\"\n",
    "\n",
    "ds_orig = MNLDataset(orig_dir, ann_dir)\n",
    "ds_diag = MNLDataset(diag_dir, ann_dir)\n",
    "full_train = torch.utils.data.ConcatDataset([ds_orig, ds_diag])\n",
    "\n",
    "# Enable cropping in diagnostic/original, keep test uncropped\n",
    "train_ds = MNLDataset(orig_dir, ann_dir, crop_whitespace=True)\n",
    "train_diag = MNLDataset(diag_dir, ann_dir, crop_whitespace=True)\n",
    "full_train = torch.utils.data.ConcatDataset([train_ds, train_diag])\n",
    "val_size = int(0.2 * len(full_train))\n",
    "train_size = len(full_train) - val_size\n",
    "train_ds, val_ds = random_split(full_train, [train_size, val_size])\n",
    "test_ds = MNLDataset(cpg_dir, ann_dir, crop_whitespace=True)\n",
    "\n",
    "# loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=2, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75f30369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "SSD                                                          [156, 4]                  --\n",
       "├─GeneralizedRCNNTransform: 1-1                              [4, 3, 320, 320]          --\n",
       "├─SSDLiteFeatureExtractorMobileNet: 1-2                      [4, 128, 1, 1]            --\n",
       "│    └─Sequential: 2-1                                       --                        --\n",
       "│    │    └─Sequential: 3-1                                  [4, 672, 20, 20]          869,096\n",
       "│    │    └─Sequential: 3-2                                  [4, 960, 10, 10]          2,102,856\n",
       "│    └─ModuleList: 2-2                                       --                        --\n",
       "│    │    └─Sequential: 3-3                                  [4, 512, 5, 5]            381,184\n",
       "│    │    └─Sequential: 3-4                                  [4, 256, 3, 3]            100,480\n",
       "│    │    └─Sequential: 3-5                                  [4, 256, 2, 2]            67,712\n",
       "│    │    └─Sequential: 3-6                                  [4, 128, 1, 1]            25,664\n",
       "├─SSDLiteHead: 1-3                                           [4, 3234, 2]              --\n",
       "│    └─SSDLiteRegressionHead: 2-3                            [4, 3234, 4]              --\n",
       "│    │    └─ModuleList: 3-7                                  --                        97,584\n",
       "│    └─SSDLiteClassificationHead: 2-4                        [4, 3234, 2]              --\n",
       "│    │    └─ModuleList: 3-8                                  --                        64,104\n",
       "├─DefaultBoxGenerator: 1-4                                   [3234, 4]                 --\n",
       "==============================================================================================================\n",
       "Total params: 3,708,680\n",
       "Trainable params: 3,708,680\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.95\n",
       "==============================================================================================================\n",
       "Input size (MB): 4.92\n",
       "Forward/backward pass size (MB): 627.72\n",
       "Params size (MB): 14.83\n",
       "Estimated Total Size (MB): 647.47\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build SSD-Lite MobileNetV3 model for MNL detection\n",
    "def get_model(num_classes=2):\n",
    "    # use pretrained MobilenetV3 backbone, random head\n",
    "    model = ssdlite320_mobilenet_v3_large(\n",
    "        weights=None,\n",
    "        weights_backbone=MobileNet_V3_Large_Weights.IMAGENET1K_V2,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    return model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = get_model(num_classes=2).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Print summary\n",
    "summary(model, input_size=[(4, 3, 320, 320)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eeb7c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6c8c1e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 40.0 GiB for an array with shape (1, 1, 155904, 91904, 3) and data type uint8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15448\\2189160289.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# training loop can include resizing and transforms similar to eval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'START BATCH: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mimgs_resized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m320\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m320\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_profile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m             if (\n\u001b[0;32m    704\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    758\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__getitems__\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[1;31m# see torch.utils.data._utils.fetch._MapDatasetFetcher\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__getitems__\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdataset_idx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             \u001b[0msample_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m             \u001b[0msample_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset_idx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15448\\2594027429.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mimg_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtifffile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;31m# ensure RGB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mimg_np\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mimg_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimg_np\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tifffile\\tifffile.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1277\u001b[0m                     \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1278\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mselection\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m                         \u001b[1;32mreturn\u001b[0m \u001b[0mstore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mzarr_selection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1281\u001b[1;33m                 return tif.asarray(\n\u001b[0m\u001b[0;32m   1282\u001b[0m                     \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1283\u001b[0m                     \u001b[0mseries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m                     \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tifffile\\tifffile.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, series, level, squeeze, out, maxworkers, buffersize)\u001b[0m\n\u001b[0;32m   4576\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4577\u001b[0m             \u001b[0mpage0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4578\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpage0\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4579\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'page is None'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4580\u001b[1;33m             result = page0.asarray(\n\u001b[0m\u001b[0;32m   4581\u001b[0m                 \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxworkers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffersize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffersize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4582\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4583\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tifffile\\tifffile.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, out, squeeze, lock, maxworkers, buffersize)\u001b[0m\n\u001b[0;32m   8886\u001b[0m                     \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8887\u001b[0m                     \u001b[0mfh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8888\u001b[0m                 \u001b[0mkeyframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m  \u001b[1;31m# init TiffPage.decode function under lock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8890\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeyframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshaped\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeyframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   8891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8892\u001b[0m             def func(\n\u001b[0;32m   8893\u001b[0m                 decoderesult: tuple[\n",
      "\u001b[1;32mc:\\Users\\luukn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tifffile\\tifffile.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(out, shape, dtype, mode, suffix, fillvalue)\u001b[0m\n\u001b[0;32m  23431\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfillvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  23432\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  23433\u001b[0m             \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfillvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  23434\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 23435\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  23436\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  23437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  23438\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'incompatible output shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 40.0 GiB for an array with shape (1, 1, 155904, 91904, 3) and data type uint8"
     ]
    }
   ],
   "source": [
    "for epoch in range(NR_EPOCHS):\n",
    "    # training loop can include resizing and transforms similar to eval\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch = 0 \n",
    "    for imgs, targets in train_loader:\n",
    "        print('START BATCH: ', batch)\n",
    "        imgs_resized = [T.Resize((320, 320))(img).to(device) for img in imgs]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(imgs_resized, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += losses.item()\n",
    "        print(f\"Epoch {epoch+1}, Batch {batch}, Loss: {losses.item():.4f}\")\n",
    "    print('---------------------------------------------------------------------------')\n",
    "    print(f\"Epoch {epoch+1:02d}, Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b1db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    for imgs, targets in loader:\n",
    "        imgs_resized = [T.Resize((320, 320))(img).to(device) for img in imgs]\n",
    "        outputs = model(imgs_resized)\n",
    "        all_predictions.extend([{k: v.cpu() for k, v in out.items()} for out in outputs])\n",
    "    return all_predictions\n",
    "\n",
    "# Evaluate on test\n",
    "preds = evaluate(model, test_loader, device)\n",
    "torch.save(preds, 'predictions_cpg_ssd.pt')\n",
    "print(\"Saved SSD predictions_cpg_ssd.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
